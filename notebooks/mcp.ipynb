{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ded50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52517f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core.agent.workflow import FunctionAgent, AgentWorkflow\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87f36aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configuration\n",
    "WORKSPACE = str(Path.home() / \"code/karpathy-gpt/src/medievallm\")\n",
    "# Make sure the Go binary is found\n",
    "MCP_BIN = str(Path.home() / \"go/bin/mcp-language-server\")\n",
    "LSP_BIN = shutil.which(\"basedpyright-langserver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdcab672",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"] = f\"{Path.home()}/go/bin:{os.environ.get('PATH', '')}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7e80a",
   "metadata": {},
   "source": [
    "# try with smolagents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18180dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import MCPClient, CodeAgent, ToolCallingAgent, InferenceClientModel\n",
    "from mcp import StdioServerParameters\n",
    "from mcp.client.streamable_http import streamablehttp_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5bbdba",
   "metadata": {},
   "source": [
    "## new mcp server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14de420c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rudy/code/smolagents/.venv/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "706ad6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool.name = 'get_commit', \n",
      "{'include_diff': {'default': True, 'description': 'Whether to include file diffs and stats in the response. Default is true.', 'type': 'boolean'}, 'owner': {'description': 'Repository owner', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'Repository name', 'type': 'string'}, 'sha': {'description': 'Commit SHA, branch name, or tag name', 'type': 'string'}}, \n",
      "Get details for a commit from a GitHub repository\n",
      "\n",
      "\n",
      "tool.name = 'get_discussion', \n",
      "{'discussionNumber': {'description': 'Discussion Number', 'type': 'number'}, 'owner': {'description': 'Repository owner', 'type': 'string'}, 'repo': {'description': 'Repository name', 'type': 'string'}}, \n",
      "Get a specific discussion by ID\n",
      "\n",
      "\n",
      "tool.name = 'get_discussion_comments', \n",
      "{'after': {'description': \"Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs.\", 'type': 'string'}, 'discussionNumber': {'description': 'Discussion Number', 'type': 'number'}, 'owner': {'description': 'Repository owner', 'type': 'string'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'Repository name', 'type': 'string'}}, \n",
      "Get comments from a discussion\n",
      "\n",
      "\n",
      "tool.name = 'get_file_contents', \n",
      "{'owner': {'description': 'Repository owner (username or organization)', 'type': 'string'}, 'path': {'default': '/', 'description': \"Path to file/directory (directories must end with a slash '/')\", 'type': 'string'}, 'ref': {'description': 'Accepts optional git refs such as `refs/tags/{tag}`, `refs/heads/{branch}` or `refs/pull/{pr_number}/head`', 'type': 'string'}, 'repo': {'description': 'Repository name', 'type': 'string'}, 'sha': {'description': 'Accepts optional commit SHA. If specified, it will be used instead of ref', 'type': 'string'}}, \n",
      "Get the contents of a file or directory from a GitHub repository\n",
      "\n",
      "\n",
      "tool.name = 'get_label', \n",
      "{'name': {'description': 'Label name.', 'type': 'string'}, 'owner': {'description': 'Repository owner (username or organization name)', 'type': 'string'}, 'repo': {'description': 'Repository name', 'type': 'string'}}, \n",
      "Get a specific label from a repository.\n",
      "\n",
      "\n",
      "tool.name = 'get_latest_release', \n",
      "{'owner': {'description': 'Repository owner', 'type': 'string'}, 'repo': {'description': 'Repository name', 'type': 'string'}}, \n",
      "Get the latest release in a GitHub repository\n",
      "\n",
      "\n",
      "tool.name = 'get_release_by_tag', \n",
      "{'owner': {'description': 'Repository owner', 'type': 'string'}, 'repo': {'description': 'Repository name', 'type': 'string'}, 'tag': {'description': \"Tag name (e.g., 'v1.0.0')\", 'type': 'string'}}, \n",
      "Get a specific release by its tag name in a GitHub repository\n",
      "\n",
      "\n",
      "tool.name = 'get_tag', \n",
      "{'owner': {'description': 'Repository owner', 'type': 'string'}, 'repo': {'description': 'Repository name', 'type': 'string'}, 'tag': {'description': 'Tag name', 'type': 'string'}}, \n",
      "Get details about a specific git tag in a GitHub repository\n",
      "\n",
      "\n",
      "tool.name = 'issue_read', \n",
      "{'issue_number': {'description': 'The number of the issue', 'type': 'number'}, 'method': {'description': 'The read operation to perform on a single issue. \\nOptions are: \\n1. get - Get details of a specific issue.\\n2. get_comments - Get issue comments.\\n3. get_sub_issues - Get sub-issues of the issue.\\n4. get_labels - Get labels assigned to the issue.\\n', 'enum': ['get', 'get_comments', 'get_sub_issues', 'get_labels'], 'type': 'string'}, 'owner': {'description': 'The owner of the repository', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'The name of the repository', 'type': 'string'}}, \n",
      "Get information about a specific issue in a GitHub repository.\n",
      "\n",
      "\n",
      "tool.name = 'list_branches', \n",
      "{'owner': {'description': 'Repository owner', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'Repository name', 'type': 'string'}}, \n",
      "List branches in a GitHub repository\n",
      "\n",
      "\n",
      "tool.name = 'list_commits', \n",
      "{'author': {'description': 'Author username or email address to filter commits by', 'type': 'string'}, 'owner': {'description': 'Repository owner', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'Repository name', 'type': 'string'}, 'sha': {'description': 'Commit SHA, branch or tag name to list commits of. If not provided, uses the default branch of the repository. If a commit SHA is provided, will list commits up to that SHA.', 'type': 'string'}}, \n",
      "Get list of commits of a branch in a GitHub repository. Returns at least 30 results per page by default, but can return more if specified using the perPage parameter (up to 100).\n",
      "\n",
      "\n",
      "tool.name = 'list_discussion_categories', \n",
      "{'owner': {'description': 'Repository owner', 'type': 'string'}, 'repo': {'description': 'Repository name. If not provided, discussion categories will be queried at the organisation level.', 'type': 'string'}}, \n",
      "List discussion categories with their id and name, for a repository or organisation.\n",
      "\n",
      "\n",
      "tool.name = 'list_discussions', \n",
      "{'after': {'description': \"Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs.\", 'type': 'string'}, 'category': {'description': 'Optional filter by discussion category ID. If provided, only discussions with this category are listed.', 'type': 'string'}, 'direction': {'description': 'Order direction.', 'enum': ['ASC', 'DESC'], 'type': 'string'}, 'orderBy': {'description': \"Order discussions by field. If provided, the 'direction' also needs to be provided.\", 'enum': ['CREATED_AT', 'UPDATED_AT'], 'type': 'string'}, 'owner': {'description': 'Repository owner', 'type': 'string'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'Repository name. If not provided, discussions will be queried at the organisation level.', 'type': 'string'}}, \n",
      "List discussions for a repository or organisation.\n",
      "\n",
      "\n",
      "tool.name = 'list_issue_types', \n",
      "{'owner': {'description': 'The organization owner of the repository', 'type': 'string'}}, \n",
      "List supported issue types for repository owner (organization).\n",
      "\n",
      "\n",
      "tool.name = 'list_issues', \n",
      "{'after': {'description': \"Cursor for pagination. Use the endCursor from the previous page's PageInfo for GraphQL APIs.\", 'type': 'string'}, 'direction': {'description': \"Order direction. If provided, the 'orderBy' also needs to be provided.\", 'enum': ['ASC', 'DESC'], 'type': 'string'}, 'labels': {'description': 'Filter by labels', 'items': {'type': 'string'}, 'type': 'array'}, 'orderBy': {'description': \"Order issues by field. If provided, the 'direction' also needs to be provided.\", 'enum': ['CREATED_AT', 'UPDATED_AT', 'COMMENTS'], 'type': 'string'}, 'owner': {'description': 'Repository owner', 'type': 'string'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'Repository name', 'type': 'string'}, 'since': {'description': 'Filter by date (ISO 8601 timestamp)', 'type': 'string'}, 'state': {'description': 'Filter by state, by default both open and closed issues are returned when not provided', 'enum': ['OPEN', 'CLOSED'], 'type': 'string'}}, \n",
      "List issues in a GitHub repository. For pagination, use the 'endCursor' from the previous response's 'pageInfo' in the 'after' parameter.\n",
      "\n",
      "\n",
      "tool.name = 'list_pull_requests', \n",
      "{'base': {'description': 'Filter by base branch', 'type': 'string'}, 'direction': {'description': 'Sort direction', 'enum': ['asc', 'desc'], 'type': 'string'}, 'head': {'description': 'Filter by head user/org and branch', 'type': 'string'}, 'owner': {'description': 'Repository owner', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'Repository name', 'type': 'string'}, 'sort': {'description': 'Sort by', 'enum': ['created', 'updated', 'popularity', 'long-running'], 'type': 'string'}, 'state': {'description': 'Filter by state', 'enum': ['open', 'closed', 'all'], 'type': 'string'}}, \n",
      "List pull requests in a GitHub repository. If the user specifies an author, then DO NOT use this tool and use the search_pull_requests tool instead.\n",
      "\n",
      "\n",
      "tool.name = 'list_releases', \n",
      "{'owner': {'description': 'Repository owner', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'Repository name', 'type': 'string'}}, \n",
      "List releases in a GitHub repository\n",
      "\n",
      "\n",
      "tool.name = 'list_tags', \n",
      "{'owner': {'description': 'Repository owner', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'repo': {'description': 'Repository name', 'type': 'string'}}, \n",
      "List git tags in a GitHub repository\n",
      "\n",
      "\n",
      "tool.name = 'pull_request_read', \n",
      "{'method': {'description': \"Action to specify what pull request data needs to be retrieved from GitHub. \\nPossible options: \\n 1. get - Get details of a specific pull request.\\n 2. get_diff - Get the diff of a pull request.\\n 3. get_status - Get status of a head commit in a pull request. This reflects status of builds and checks.\\n 4. get_files - Get the list of files changed in a pull request. Use with pagination parameters to control the number of results returned.\\n 5. get_review_comments - Get the review comments on a pull request. They are comments made on a portion of the unified diff during a pull request review. Use with pagination parameters to control the number of results returned.\\n 6. get_reviews - Get the reviews on a pull request. When asked for review comments, use get_review_comments method.\\n 7. get_comments - Get comments on a pull request. Use this if user doesn't specifically want review comments. Use with pagination parameters to control the number of results returned.\\n\", 'enum': ['get', 'get_diff', 'get_status', 'get_files', 'get_review_comments', 'get_reviews', 'get_comments'], 'type': 'string'}, 'owner': {'description': 'Repository owner', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'pullNumber': {'description': 'Pull request number', 'type': 'number'}, 'repo': {'description': 'Repository name', 'type': 'string'}}, \n",
      "Get information on a specific pull request in GitHub repository.\n",
      "\n",
      "\n",
      "tool.name = 'search_code', \n",
      "{'order': {'description': 'Sort order for results', 'enum': ['asc', 'desc'], 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'query': {'description': \"Search query using GitHub's powerful code search syntax. Examples: 'content:Skill language:Java org:github', 'NOT is:archived language:Python OR language:go', 'repo:github/github-mcp-server'. Supports exact matching, language filters, path filters, and more.\", 'type': 'string'}, 'sort': {'description': \"Sort field ('indexed' only)\", 'type': 'string'}}, \n",
      "Fast and precise code search across ALL GitHub repositories using GitHub's native search engine. Best for finding exact symbols, functions, classes, or specific code patterns.\n",
      "\n",
      "\n",
      "tool.name = 'search_issues', \n",
      "{'order': {'description': 'Sort order', 'enum': ['asc', 'desc'], 'type': 'string'}, 'owner': {'description': 'Optional repository owner. If provided with repo, only issues for this repository are listed.', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'query': {'description': 'Search query using GitHub issues search syntax', 'type': 'string'}, 'repo': {'description': 'Optional repository name. If provided with owner, only issues for this repository are listed.', 'type': 'string'}, 'sort': {'description': 'Sort field by number of matches of categories, defaults to best match', 'enum': ['comments', 'reactions', 'reactions-+1', 'reactions--1', 'reactions-smile', 'reactions-thinking_face', 'reactions-heart', 'reactions-tada', 'interactions', 'created', 'updated'], 'type': 'string'}}, \n",
      "Search for issues in GitHub repositories using issues search syntax already scoped to is:issue\n",
      "\n",
      "\n",
      "tool.name = 'search_pull_requests', \n",
      "{'order': {'description': 'Sort order', 'enum': ['asc', 'desc'], 'type': 'string'}, 'owner': {'description': 'Optional repository owner. If provided with repo, only pull requests for this repository are listed.', 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'query': {'description': 'Search query using GitHub pull request search syntax', 'type': 'string'}, 'repo': {'description': 'Optional repository name. If provided with owner, only pull requests for this repository are listed.', 'type': 'string'}, 'sort': {'description': 'Sort field by number of matches of categories, defaults to best match', 'enum': ['comments', 'reactions', 'reactions-+1', 'reactions--1', 'reactions-smile', 'reactions-thinking_face', 'reactions-heart', 'reactions-tada', 'interactions', 'created', 'updated'], 'type': 'string'}}, \n",
      "Search for pull requests in GitHub repositories using issues search syntax already scoped to is:pr\n",
      "\n",
      "\n",
      "tool.name = 'search_repositories', \n",
      "{'minimal_output': {'default': True, 'description': 'Return minimal repository information (default: true). When false, returns full GitHub API repository objects.', 'type': 'boolean'}, 'order': {'description': 'Sort order', 'enum': ['asc', 'desc'], 'type': 'string'}, 'page': {'description': 'Page number for pagination (min 1)', 'minimum': 1, 'type': 'number'}, 'perPage': {'description': 'Results per page for pagination (min 1, max 100)', 'maximum': 100, 'minimum': 1, 'type': 'number'}, 'query': {'description': \"Repository search query. Examples: 'machine learning in:name stars:>1000 language:python', 'topic:react', 'user:facebook'. Supports advanced search syntax for precise filtering.\", 'type': 'string'}, 'sort': {'description': 'Sort repositories by field, defaults to best match', 'enum': ['stars', 'forks', 'help-wanted-issues', 'updated'], 'type': 'string'}}, \n",
      "Find GitHub repositories by name, description, readme, topics, or other metadata. Perfect for discovering projects, finding examples, or locating specific repositories across GitHub.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = MCPClient(\n",
    "    {\n",
    "        \"url\": \"https://api.githubcopilot.com/mcp/\",\n",
    "        \"transport\": \"streamable-http\",\n",
    "        \"headers\": {\n",
    "            \"Authorization\": f\"Bearer {os.environ['GITHUB_PAT']}\",\n",
    "            \"X-MCP-Toolsets\": \"discussions,issues,pull_requests,repos\",\n",
    "            \"X-MCP-Readonly\": \"true\",\n",
    "            \"X-MCP-Lockdown\": \"false\",\n",
    "        },\n",
    "    },\n",
    "    structured_output=True,\n",
    ")\n",
    "for tool in client.get_tools():\n",
    "    print(f\"{tool.name = }, \\n{tool.inputs}, \\n{tool.description}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db91dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InferenceClientModel(\n",
    "    \"Qwen/Qwen3-Coder-30B-A3B-Instruct\",\n",
    "    provider=\"nebius\",\n",
    "    api_key=os.environ[\"NEBIUS_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84e01381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">You are an agent who is provided with tools regarding the github mcp server. Use a single tool call to give me </span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">brief results about the huggingface transformers repository</span>                                                     <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - Qwen/Qwen3-Coder-30B-A3B-Instruct ──────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mYou are an agent who is provided with tools regarding the github mcp server. Use a single tool call to give me \u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mbrief results about the huggingface transformers repository\u001b[0m                                                     \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - Qwen/Qwen3-Coder-30B-A3B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'search_repositories' with arguments: {'minimal_output': True, 'order': 'desc', 'page': 1,        │\n",
       "│ 'perPage': 1, 'query': 'huggingface transformers', 'sort': 'stars'}                                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'search_repositories' with arguments: {'minimal_output': True, 'order': 'desc', 'page': 1,        │\n",
       "│ 'perPage': 1, 'query': 'huggingface transformers', 'sort': 'stars'}                                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: {'total_count': 3048, 'incomplete_results': False, 'items': |{'id': 291787503, 'name': \n",
       "'Transformers-Tutorials', 'full_name': 'NielsRogge/Transformers-Tutorials', 'description': 'This repository \n",
       "contains demos I made with the Transformers library by HuggingFace.', 'html_url': \n",
       "'https://github.com/NielsRogge/Transformers-Tutorials', 'language': 'Jupyter Notebook', 'stargazers_count': 11369, \n",
       "'forks_count': 1689, 'open_issues_count': 326, 'updated_at': '2025-11-24T12:49:55Z', 'created_at': \n",
       "'2020-08-31T18:00:58Z', 'topics': |'bert', 'gpt-2', 'layoutlm', 'pytorch', 'transformers', 'vision-transformer'], \n",
       "'private': False, 'fork': False, 'archived': False, 'default_branch': 'master'}]}\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: {'total_count': 3048, 'incomplete_results': False, 'items': |{'id': 291787503, 'name': \n",
       "'Transformers-Tutorials', 'full_name': 'NielsRogge/Transformers-Tutorials', 'description': 'This repository \n",
       "contains demos I made with the Transformers library by HuggingFace.', 'html_url': \n",
       "'https://github.com/NielsRogge/Transformers-Tutorials', 'language': 'Jupyter Notebook', 'stargazers_count': 11369, \n",
       "'forks_count': 1689, 'open_issues_count': 326, 'updated_at': '2025-11-24T12:49:55Z', 'created_at': \n",
       "'2020-08-31T18:00:58Z', 'topics': |'bert', 'gpt-2', 'layoutlm', 'pytorch', 'transformers', 'vision-transformer'], \n",
       "'private': False, 'fork': False, 'archived': False, 'default_branch': 'master'}]}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 21.84 seconds| Input tokens: 19,704 | Output tokens: 53]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 21.84 seconds| Input tokens: 19,704 | Output tokens: 53]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'search_repositories' with arguments: {'minimal_output': True, 'order': 'desc', 'page': 1,        │\n",
       "│ 'perPage': 1, 'query': 'huggingface transformers', 'sort': 'stars'}                                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'search_repositories' with arguments: {'minimal_output': True, 'order': 'desc', 'page': 1,        │\n",
       "│ 'perPage': 1, 'query': 'huggingface transformers', 'sort': 'stars'}                                             │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: {'total_count': 3048, 'incomplete_results': False, 'items': |{'id': 291787503, 'name': \n",
       "'Transformers-Tutorials', 'full_name': 'NielsRogge/Transformers-Tutorials', 'description': 'This repository \n",
       "contains demos I made with the Transformers library by HuggingFace.', 'html_url': \n",
       "'https://github.com/NielsRogge/Transformers-Tutorials', 'language': 'Jupyter Notebook', 'stargazers_count': 11369, \n",
       "'forks_count': 1689, 'open_issues_count': 326, 'updated_at': '2025-11-24T12:49:55Z', 'created_at': \n",
       "'2020-08-31T18:00:58Z', 'topics': |'bert', 'gpt-2', 'layoutlm', 'pytorch', 'transformers', 'vision-transformer'], \n",
       "'private': False, 'fork': False, 'archived': False, 'default_branch': 'master'}]}\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: {'total_count': 3048, 'incomplete_results': False, 'items': |{'id': 291787503, 'name': \n",
       "'Transformers-Tutorials', 'full_name': 'NielsRogge/Transformers-Tutorials', 'description': 'This repository \n",
       "contains demos I made with the Transformers library by HuggingFace.', 'html_url': \n",
       "'https://github.com/NielsRogge/Transformers-Tutorials', 'language': 'Jupyter Notebook', 'stargazers_count': 11369, \n",
       "'forks_count': 1689, 'open_issues_count': 326, 'updated_at': '2025-11-24T12:49:55Z', 'created_at': \n",
       "'2020-08-31T18:00:58Z', 'topics': |'bert', 'gpt-2', 'layoutlm', 'pytorch', 'transformers', 'vision-transformer'], \n",
       "'private': False, 'fork': False, 'archived': False, 'default_branch': 'master'}]}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 5.02 seconds| Input tokens: 39,769 | Output tokens: 106]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 5.02 seconds| Input tokens: 39,769 | Output tokens: 106]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The Hugging Face Transformers repository is actually a │\n",
       "│ collection of tutorials rather than the main library itself. Here are the key details about the                 │\n",
       "│ Transformers-Tutorials repository by NielsRogge:\\n\\n- Repository Name: Transformers-Tutorials\\n- Description:   │\n",
       "│ This repository contains demos made with the Transformers library by HuggingFace\\n- Stars: 11,369\\n- Forks:     │\n",
       "│ 1,689\\n- Open Issues: 326\\n- Language: Jupyter Notebook\\n- Topics: bert, gpt-2, layoutlm, pytorch,              │\n",
       "│ transformers, vision-transformer\\n- URL: https://github.com/NielsRogge/Transformers-Tutorials\\n\\nNote: The      │\n",
       "│ official Hugging Face Transformers library repository is likely named just \"transformers\" or \"transformers\"     │\n",
       "│ under the huggingface organization.'}                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The Hugging Face Transformers repository is actually a │\n",
       "│ collection of tutorials rather than the main library itself. Here are the key details about the                 │\n",
       "│ Transformers-Tutorials repository by NielsRogge:\\n\\n- Repository Name: Transformers-Tutorials\\n- Description:   │\n",
       "│ This repository contains demos made with the Transformers library by HuggingFace\\n- Stars: 11,369\\n- Forks:     │\n",
       "│ 1,689\\n- Open Issues: 326\\n- Language: Jupyter Notebook\\n- Topics: bert, gpt-2, layoutlm, pytorch,              │\n",
       "│ transformers, vision-transformer\\n- URL: https://github.com/NielsRogge/Transformers-Tutorials\\n\\nNote: The      │\n",
       "│ official Hugging Face Transformers library repository is likely named just \"transformers\" or \"transformers\"     │\n",
       "│ under the huggingface organization.'}                                                                           │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: The Hugging Face Transformers repository is actually a collection of tutorials rather than the main \n",
       "library itself. Here are the key details about the Transformers-Tutorials repository by NielsRogge:\n",
       "\n",
       "- Repository Name: Transformers-Tutorials\n",
       "- Description: This repository contains demos made with the Transformers library by HuggingFace\n",
       "- Stars: 11,369\n",
       "- Forks: 1,689\n",
       "- Open Issues: 326\n",
       "- Language: Jupyter Notebook\n",
       "- Topics: bert, gpt-2, layoutlm, pytorch, transformers, vision-transformer\n",
       "- URL: https://github.com/NielsRogge/Transformers-Tutorials\n",
       "\n",
       "Note: The official Hugging Face Transformers library repository is likely named just \"transformers\" or \n",
       "\"transformers\" under the huggingface organization.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: The Hugging Face Transformers repository is actually a collection of tutorials rather than the main \n",
       "library itself. Here are the key details about the Transformers-Tutorials repository by NielsRogge:\n",
       "\n",
       "- Repository Name: Transformers-Tutorials\n",
       "- Description: This repository contains demos made with the Transformers library by HuggingFace\n",
       "- Stars: 11,369\n",
       "- Forks: 1,689\n",
       "- Open Issues: 326\n",
       "- Language: Jupyter Notebook\n",
       "- Topics: bert, gpt-2, layoutlm, pytorch, transformers, vision-transformer\n",
       "- URL: https://github.com/NielsRogge/Transformers-Tutorials\n",
       "\n",
       "Note: The official Hugging Face Transformers library repository is likely named just \"transformers\" or \n",
       "\"transformers\" under the huggingface organization.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The Hugging Face Transformers repository is actually a collection of tutorials rather than the main </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">library itself. Here are the key details about the Transformers-Tutorials repository by NielsRogge:</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Repository Name: Transformers-Tutorials</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Description: This repository contains demos made with the Transformers library by HuggingFace</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Stars: 11,369</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Forks: 1,689</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Open Issues: 326</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Language: Jupyter Notebook</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- Topics: bert, gpt-2, layoutlm, pytorch, transformers, vision-transformer</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- URL: https://github.com/NielsRogge/Transformers-Tutorials</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Note: The official Hugging Face Transformers library repository is likely named just \"transformers\" or </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">\"transformers\" under the huggingface organization.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The Hugging Face Transformers repository is actually a collection of tutorials rather than the main \u001b[0m\n",
       "\u001b[1;38;2;212;183;2mlibrary itself. Here are the key details about the Transformers-Tutorials repository by NielsRogge:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- Repository Name: Transformers-Tutorials\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Description: This repository contains demos made with the Transformers library by HuggingFace\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Stars: 11,369\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Forks: 1,689\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Open Issues: 326\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Language: Jupyter Notebook\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- Topics: bert, gpt-2, layoutlm, pytorch, transformers, vision-transformer\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- URL: https://github.com/NielsRogge/Transformers-Tutorials\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2mNote: The official Hugging Face Transformers library repository is likely named just \"transformers\" or \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m\"transformers\" under the huggingface organization.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 9.60 seconds| Input tokens: 60,192 | Output tokens: 298]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 9.60 seconds| Input tokens: 60,192 | Output tokens: 298]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The Hugging Face Transformers repository is actually a collection of tutorials rather than the main library itself. Here are the key details about the Transformers-Tutorials repository by NielsRogge:\\n\\n- Repository Name: Transformers-Tutorials\\n- Description: This repository contains demos made with the Transformers library by HuggingFace\\n- Stars: 11,369\\n- Forks: 1,689\\n- Open Issues: 326\\n- Language: Jupyter Notebook\\n- Topics: bert, gpt-2, layoutlm, pytorch, transformers, vision-transformer\\n- URL: https://github.com/NielsRogge/Transformers-Tutorials\\n\\nNote: The official Hugging Face Transformers library repository is likely named just \"transformers\" or \"transformers\" under the huggingface organization.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = ToolCallingAgent(tools=client.get_tools(), model=model, stream_outputs=False)\n",
    "agent.run(\n",
    "    \"You are an agent who is provided with tools regarding the github mcp server. Use a single tool call to give me brief results about the huggingface transformers repository\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bab6957e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StdioServerParameters(command='npx', args=['-y', '@modelcontextprotocol/server-filesystem', '/home/rudy/code/smolagents'], env=None, cwd=None, encoding='utf-8', encoding_error_handler='strict')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StdioServerParameters(\n",
    "    command=\"npx\",\n",
    "    args=[\n",
    "        \"-y\",\n",
    "        \"@modelcontextprotocol/server-filesystem\",\n",
    "        f\"{Path.home() / 'code' / 'smolagents'}\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83a22b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4085/324223350.py:1: FutureWarning: Parameter 'structured_output' was not specified. Currently it defaults to False, but in version 1.25, the default will change to True. To suppress this warning, explicitly set structured_output=True (new behavior) or structured_output=False (legacy behavior). See documentation at https://huggingface.co/docs/smolagents/tutorials/tools#structured-output-and-output-schema-support for more details.\n",
      "  client = MCPClient(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'properties'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m client = \u001b[43mMCPClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mStdioServerParameters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnpx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-y\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m@modelcontextprotocol/server-filesystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhome\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m/\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcode\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m/\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msmolagents\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m client.get_tools():\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool.name\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtool.inputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtool.description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/smolagents/mcp_client.py:122\u001b[39m, in \u001b[36mMCPClient.__init__\u001b[39m\u001b[34m(self, server_parameters, adapter_kwargs, structured_output)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m._adapter = MCPAdapt(\n\u001b[32m    119\u001b[39m     server_parameters, SmolAgentsAdapter(structured_output=structured_output), **adapter_kwargs\n\u001b[32m    120\u001b[39m )\n\u001b[32m    121\u001b[39m \u001b[38;5;28mself\u001b[39m._tools: \u001b[38;5;28mlist\u001b[39m[Tool] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/smolagents/mcp_client.py:126\u001b[39m, in \u001b[36mMCPClient.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    125\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Connect to the MCP server and initialize the tools.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28mself\u001b[39m._tools: \u001b[38;5;28mlist\u001b[39m[Tool] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_adapter\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__enter__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/mcpadapt/core.py:311\u001b[39m, in \u001b[36mMCPAdapt.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    310\u001b[39m     \u001b[38;5;28mself\u001b[39m.start()\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/mcpadapt/core.py:286\u001b[39m, in \u001b[36mMCPAdapt.tools\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    277\u001b[39m     mcp_tools.extend(\n\u001b[32m    278\u001b[39m         [\n\u001b[32m    279\u001b[39m             asyncio.run_coroutine_threadsafe(\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m         ]\n\u001b[32m    283\u001b[39m     )\n\u001b[32m    284\u001b[39m \u001b[38;5;28mself\u001b[39m.mcp_tools = mcp_tools\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_sync_call_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msessions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmcp_tools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/mcpadapt/core.py:287\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    277\u001b[39m     mcp_tools.extend(\n\u001b[32m    278\u001b[39m         [\n\u001b[32m    279\u001b[39m             asyncio.run_coroutine_threadsafe(\n\u001b[32m   (...)\u001b[39m\u001b[32m    282\u001b[39m         ]\n\u001b[32m    283\u001b[39m     )\n\u001b[32m    284\u001b[39m \u001b[38;5;28mself\u001b[39m.mcp_tools = mcp_tools\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43madapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_sync_call_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m session, tools \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.sessions, \u001b[38;5;28mself\u001b[39m.mcp_tools)\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools\n\u001b[32m    290\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/mcpadapt/smolagents_adapter.py:103\u001b[39m, in \u001b[36mSmolAgentsAdapter.adapt\u001b[39m\u001b[34m(self, func, mcp_tool)\u001b[39m\n\u001b[32m     96\u001b[39m input_schema = {\n\u001b[32m     97\u001b[39m     k: v\n\u001b[32m     98\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m jsonref.replace_refs(mcp_tool.inputSchema).items()\n\u001b[32m     99\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[33m\"\u001b[39m\u001b[33m$defs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m }\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# make sure mandatory `description` and `type` is provided for each arguments:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43minput_schema\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mproperties\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.items():\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m v:\n\u001b[32m    105\u001b[39m         input_schema[\u001b[33m\"\u001b[39m\u001b[33mproperties\u001b[39m\u001b[33m\"\u001b[39m][k][\u001b[33m\"\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33msee tool description\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'properties'"
     ]
    }
   ],
   "source": [
    "client = MCPClient(\n",
    "    StdioServerParameters(\n",
    "        command=\"npx\",\n",
    "        args=[\n",
    "            \"-y\",\n",
    "            \"@modelcontextprotocol/server-filesystem\",\n",
    "            f\"{Path.home() / 'code' / 'smolagents'}\",\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "for tool in client.get_tools():\n",
    "    print(f\"{tool.name = }, \\n{tool.inputs}, \\n{tool.description}\\n\\n\")\n",
    "model = InferenceClientModel(\n",
    "    \"Qwen/Qwen3-Coder-30B-A3B-Instruct\",\n",
    "    provider=\"nebius\",\n",
    "    api_key=os.environ[\"NEBIUS_API_KEY\"],\n",
    ")\n",
    "agent = ToolCallingAgent(tools=client.get_tools(), model=model, stream_outputs=False)\n",
    "agent.run(\n",
    "    \"You are an agent who is provided with tools regarding the filesystem where the current directory is mounted. Just tell me what mcp related files are present, without multiple tool calls.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62cae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool.name = 'definition_short', \n",
      "{'symbol': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'file_path': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'max_chars': {'default': 1000, 'type': 'integer', 'description': 'see tool description'}}, \n",
      "Retrieve concise info about a symbol limited to name, docstring and location.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "symbol : str, optional\n",
      "    Symbol name or qualified name. Takes precedence over `file_path`.\n",
      "file_path : str, optional\n",
      "    Python file path if `symbol` is not given.\n",
      "max_chars : int, optional\n",
      "    Maximum docstring length (default: 1000).\n",
      "\n",
      "\n",
      "tool.name = 'definition_full', \n",
      "{'symbol': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'file_path': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'max_chars': {'default': 5000, 'type': 'integer', 'description': 'see tool description'}}, \n",
      "Retrieve full source and metadata for a symbol or module.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "symbol : str, optional\n",
      "    Symbol name or qualified name. Takes precedence over `file_path`.\n",
      "file_path : str, optional\n",
      "    Python file path if `symbol` is not given.\n",
      "max_chars : int, optional\n",
      "    Maximum source length (default: 5000).\n",
      "\n",
      "\n",
      "tool.name = 'outline', \n",
      "{'symbol': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'file_path': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'max_chars': {'default': 1000, 'type': 'integer', 'description': 'see tool description'}}, \n",
      "List symbols and structure under a symbol or module. WARNING: Large output content.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "symbol : str, optional\n",
      "    Symbol name or qualified name.\n",
      "file_path : str, optional\n",
      "    Python file path if `symbol` is not given.\n",
      "max_chars : int, optional\n",
      "    Maximum docstring length (default: 1000).\n",
      "\n",
      "\n",
      "tool.name = 'references', \n",
      "{'symbol': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'file_path': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'max_chars': {'default': 300, 'type': 'integer', 'description': 'see tool description'}}, \n",
      "List references for a symbol or file.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "symbol : str, optional\n",
      "    Symbol name for which to retrieve references.\n",
      "file_path : str, optional\n",
      "    Python file path if `symbol` is not given.\n",
      "max_chars : int, optional\n",
      "    Maximum context length (default: 300).\n",
      "\n",
      "\n",
      "tool.name = 'filter_symbols', \n",
      "{'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'kind': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'type_hint': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'see tool description', 'type': 'string'}, 'max_results': {'default': 50, 'type': 'integer', 'description': 'see tool description'}, 'max_chars': {'default': 500, 'type': 'integer', 'description': 'see tool description'}}, \n",
      "Filter symbols by name, kind, or type hint.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "name : str, optional\n",
      "    Substring or regex in symbol name.\n",
      "kind : str, optional\n",
      "    Symbol kind ('class', 'function', etc.).\n",
      "type_hint : str, optional\n",
      "    Substring in type annotation.\n",
      "max_results : int, optional\n",
      "    Maximum number of results (default: 50).\n",
      "max_chars : int, optional\n",
      "    Maximum docstring length (default: 500).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = MCPClient(\n",
    "    StdioServerParameters(\n",
    "        command=sys.executable,\n",
    "        args=[\n",
    "            f\"{Path.home() / 'code' / 'smolagents' / 'mcp' / 'python-lsp-mcp-server' / 'src' / 'server.py'}\",\n",
    "            \"--project-root\",\n",
    "            f\"{Path.home() / 'code' / 'karpathy-gpt'}\",\n",
    "            \"--transport\",\n",
    "            \"stdio\",\n",
    "        ],\n",
    "        env={\n",
    "            **os.environ,  # ✅ inherit current venv environment variables\n",
    "        },\n",
    "    ),\n",
    "    structured_output=True,\n",
    ")\n",
    "for tool in client.get_tools():\n",
    "    print(f\"{tool.name = }, \\n{tool.inputs}, \\n{tool.description}\\n\\n\")\n",
    "model = InferenceClientModel(\n",
    "    \"Qwen/Qwen3-Coder-30B-A3B-Instruct\",\n",
    "    provider=\"nebius\",\n",
    "    api_key=os.environ[\"NEBIUS_API_KEY\"],\n",
    ")\n",
    "agent = ToolCallingAgent(tools=client.get_tools(), model=model, stream_outputs=False)\n",
    "agent.run(\"You are an agent who is provided with \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57671154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a8f7a7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path(\"/home/rudy/code/karpathy-gpt/\").resolve()\n",
    "server_root = Path(\"/home/rudy/code/smolagents/mcp/python-lsp-mcp-server\").resolve()\n",
    "\n",
    "params = StdioServerParameters(\n",
    "    command=sys.executable,\n",
    "    args=[\n",
    "        f\"{server_root / 'src' / 'server.py'}\",\n",
    "        \"--project-root\",\n",
    "        str(project_root),\n",
    "        \"--transport\",\n",
    "        \"stdio\",\n",
    "    ],\n",
    "    env={\n",
    "        **os.environ,  # ✅ inherit current venv environment variables\n",
    "    },\n",
    ")\n",
    "model = InferenceClientModel(\n",
    "    \"Qwen/Qwen3-Coder-30B-A3B-Instruct\",\n",
    "    provider=\"nebius\",\n",
    "    api_key=os.environ[\"NEBIUS_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d952ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What parameters does MedievalLMTokenizer's init accept?</span>                                                         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - Qwen/Qwen3-Coder-30B-A3B-Instruct ──────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat parameters does MedievalLMTokenizer's init accept?\u001b[0m                                                         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - Qwen/Qwen3-Coder-30B-A3B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'definition_short' with arguments: {'symbol': 'MedievalLMTokenizer', 'file_path': '',             │\n",
       "│ 'max_chars': 1000}                                                                                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'definition_short' with arguments: {'symbol': 'MedievalLMTokenizer', 'file_path': '',             │\n",
       "│ 'max_chars': 1000}                                                                                              │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: {'result': {'name': 'MedievalLMTokenizer', 'qualified_name': \n",
       "'src.medievallm.tokenizer.MedievalLMTokenizer', 'kind': 'class', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py', 'line': 13, 'column': 0, 'docstring': 'Minimal refactor\n",
       "of the provided BPE training snippet into a class.\\nPreserves the original algorithm/logic exactly while exposing \n",
       "configuration.\\n\\nUsage:\\n    trainer = MedievalLMTokenizer(text)\\n    vocab, vocab_to_str, merged_pairs = \n",
       "trainer.train()', 'init_docstring': 'Parameters mirror the original snippet, defaults set to your example.\\n- text:\n",
       "input text to train BPE on\\n- min_merge_threshold: minimum pair frequency to continue merging\\n- \n",
       "max_token_threshold: target vocab size (stop when vocab reaches this)\\n- splitting_pattern: compiled regex pattern \n",
       "to split text into subwords\\n- special_tokens: optional list of special tokens to append at the end\\n- base_path: \n",
       "optional path to load the tokenizer from\\n- verbose: if True prints the same status lines as the original \n",
       "snippet'}}\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: {'result': {'name': 'MedievalLMTokenizer', 'qualified_name': \n",
       "'src.medievallm.tokenizer.MedievalLMTokenizer', 'kind': 'class', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py', 'line': 13, 'column': 0, 'docstring': 'Minimal refactor\n",
       "of the provided BPE training snippet into a class.\\nPreserves the original algorithm/logic exactly while exposing \n",
       "configuration.\\n\\nUsage:\\n    trainer = MedievalLMTokenizer(text)\\n    vocab, vocab_to_str, merged_pairs = \n",
       "trainer.train()', 'init_docstring': 'Parameters mirror the original snippet, defaults set to your example.\\n- text:\n",
       "input text to train BPE on\\n- min_merge_threshold: minimum pair frequency to continue merging\\n- \n",
       "max_token_threshold: target vocab size (stop when vocab reaches this)\\n- splitting_pattern: compiled regex pattern \n",
       "to split text into subwords\\n- special_tokens: optional list of special tokens to append at the end\\n- base_path: \n",
       "optional path to load the tokenizer from\\n- verbose: if True prints the same status lines as the original \n",
       "snippet'}}\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 2.57 seconds| Input tokens: 3,526 | Output tokens: 37]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 2.57 seconds| Input tokens: 3,526 | Output tokens: 37]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"MedievalLMTokenizer's __init__ method accepts the      │\n",
       "│ following parameters:\\n- text: input text to train BPE on\\n- min_merge_threshold: minimum pair frequency to     │\n",
       "│ continue merging\\n- max_token_threshold: target vocab size (stop when vocab reaches this)\\n- splitting_pattern: │\n",
       "│ compiled regex pattern to split text into subwords\\n- special_tokens: optional list of special tokens to append │\n",
       "│ at the end\\n- base_path: optional path to load the tokenizer from\\n- verbose: if True prints the same status    │\n",
       "│ lines as the original snippet\"}                                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': \"MedievalLMTokenizer's __init__ method accepts the      │\n",
       "│ following parameters:\\n- text: input text to train BPE on\\n- min_merge_threshold: minimum pair frequency to     │\n",
       "│ continue merging\\n- max_token_threshold: target vocab size (stop when vocab reaches this)\\n- splitting_pattern: │\n",
       "│ compiled regex pattern to split text into subwords\\n- special_tokens: optional list of special tokens to append │\n",
       "│ at the end\\n- base_path: optional path to load the tokenizer from\\n- verbose: if True prints the same status    │\n",
       "│ lines as the original snippet\"}                                                                                 │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: MedievalLMTokenizer's __init__ method accepts the following parameters:\n",
       "- text: input text to train BPE on\n",
       "- min_merge_threshold: minimum pair frequency to continue merging\n",
       "- max_token_threshold: target vocab size (stop when vocab reaches this)\n",
       "- splitting_pattern: compiled regex pattern to split text into subwords\n",
       "- special_tokens: optional list of special tokens to append at the end\n",
       "- base_path: optional path to load the tokenizer from\n",
       "- verbose: if True prints the same status lines as the original snippet\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: MedievalLMTokenizer's __init__ method accepts the following parameters:\n",
       "- text: input text to train BPE on\n",
       "- min_merge_threshold: minimum pair frequency to continue merging\n",
       "- max_token_threshold: target vocab size (stop when vocab reaches this)\n",
       "- splitting_pattern: compiled regex pattern to split text into subwords\n",
       "- special_tokens: optional list of special tokens to append at the end\n",
       "- base_path: optional path to load the tokenizer from\n",
       "- verbose: if True prints the same status lines as the original snippet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: MedievalLMTokenizer's __init__ method accepts the following parameters:</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- text: input text to train BPE on</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- min_merge_threshold: minimum pair frequency to continue merging</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- max_token_threshold: target vocab size (stop when vocab reaches this)</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- splitting_pattern: compiled regex pattern to split text into subwords</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- special_tokens: optional list of special tokens to append at the end</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- base_path: optional path to load the tokenizer from</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- verbose: if True prints the same status lines as the original snippet</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: MedievalLMTokenizer's __init__ method accepts the following parameters:\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- text: input text to train BPE on\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- min_merge_threshold: minimum pair frequency to continue merging\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- max_token_threshold: target vocab size (stop when vocab reaches this)\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- splitting_pattern: compiled regex pattern to split text into subwords\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- special_tokens: optional list of special tokens to append at the end\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- base_path: optional path to load the tokenizer from\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- verbose: if True prints the same status lines as the original snippet\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.89 seconds| Input tokens: 7,402 | Output tokens: 164]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.89 seconds| Input tokens: 7,402 | Output tokens: 164]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with MCPClient(params, structured_output=True) as tools:\n",
    "    agent = ToolCallingAgent(tools=tools, model=model)\n",
    "    agent.run(\"What parameters does MedievalLMTokenizer's init accept?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd89943a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Give me a list of the functions defined in this project. I want only function names and do not require </span>         <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">metadata. Use the filter tool.</span>                                                                                  <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - Qwen/Qwen3-Coder-30B-A3B-Instruct ──────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mGive me a list of the functions defined in this project. I want only function names and do not require \u001b[0m         \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mmetadata. Use the filter tool.\u001b[0m                                                                                  \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - Qwen/Qwen3-Coder-30B-A3B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Let's first check what symbols are available in this project</span><span style=\"background-color: #272822\">                                                 </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">symbols </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> outline(file_path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\".\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, max_chars</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">1000</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Available symbols in the project:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                     </span>  \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> symbol </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> symbols[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'result'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]:</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"- {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">symbol</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">get(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'name'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'Unknown'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">} ({</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">symbol</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">get(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'kind'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'Unknown'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">})\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                              </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Let's first check what symbols are available in this project\u001b[0m\u001b[48;2;39;40;34m                                                 \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34msymbols\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34moutline\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile_path\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmax_chars\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m1000\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mAvailable symbols in the project:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                     \u001b[0m  \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msymbol\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msymbols\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresult\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m- \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msymbol\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mname\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mUnknown\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m (\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msymbol\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mget\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mkind\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mUnknown\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m)\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                              \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tool outline expected structured output but got unparseable text: Error calling tool 'outline': [Errno 21] Is a directory: '/home/rudy/code/smolagents/notebooks'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Available symbols in the project:\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Available symbols in the project:\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed at line 'for symbol in symbols['result'\\]:</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">    print(f\"- {symbol.get('name', 'Unknown')} ({symbol.get('kind', 'Unknown')})\")' due to: InterpreterError: Could </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">not index Error calling tool 'outline': [Errno 21\\] Is a directory: '/home/rudy/code/smolagents/notebooks' with </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'result': TypeError: string indices must be integers, not 'str'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mCode execution failed at line 'for symbol in symbols['result'\\]:\u001b[0m\n",
       "\u001b[1;31m    print(f\"- {symbol.get('name', 'Unknown')} ({symbol.get('kind', 'Unknown')})\")' due to: InterpreterError: Could \u001b[0m\n",
       "\u001b[1;31mnot index Error calling tool 'outline': [Errno 21\\] Is a directory: '/home/rudy/code/smolagents/notebooks' with \u001b[0m\n",
       "\u001b[1;31m'result': TypeError: string indices must be integers, not 'str'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 4.09 seconds| Input tokens: 3,426 | Output tokens: 162]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 4.09 seconds| Input tokens: 3,426 | Output tokens: 162]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">import</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> os</span><span style=\"background-color: #272822\">                                                                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># List all Python files in the current directory</span><span style=\"background-color: #272822\">                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">python_files </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> []</span><span style=\"background-color: #272822\">                                                                                              </span>  \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> root, dirs, files </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> os</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">walk(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\".\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">):</span><span style=\"background-color: #272822\">                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> file </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> files:</span><span style=\"background-color: #272822\">                                                                                         </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">        </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> file</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">endswith(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'.py'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">):</span><span style=\"background-color: #272822\">                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">            python_files</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">append(os</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">path</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">join(root, file))</span><span style=\"background-color: #272822\">                                                      </span>  \n",
       "  <span style=\"background-color: #272822\">                                                                                                               </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Python files found:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                   </span>  \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> f </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> python_files[:</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">10</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]:  </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Show first 10 files to avoid overwhelming output</span><span style=\"background-color: #272822\">                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"  {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">f</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">}\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                            </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">f\"... and {</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">len(python_files) </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">-</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">10</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">} more files\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">if</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> len(python_files) </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">&gt;</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">10</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">else</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                        </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;255;70;137;48;2;39;40;34mimport\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mos\u001b[0m\u001b[48;2;39;40;34m                                                                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# List all Python files in the current directory\u001b[0m\u001b[48;2;39;40;34m                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mpython_files\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                                                              \u001b[0m  \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mroot\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdirs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfiles\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mos\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwalk\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m.\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfiles\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                         \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m        \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mendswith\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m.py\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m            \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpython_files\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mappend\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mos\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpath\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mjoin\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mroot\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfile\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                      \u001b[0m  \n",
       "  \u001b[48;2;39;40;34m                                                                                                               \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mPython files found:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                   \u001b[0m  \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mf\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpython_files\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m10\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m  \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Show first 10 files to avoid overwhelming output\u001b[0m\u001b[48;2;39;40;34m                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m  \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                            \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mf\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m... and \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m{\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlen\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpython_files\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m-\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m10\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m}\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m more files\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mif\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlen\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpython_files\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m>\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m10\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34melse\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Warning to user: Code execution failed due to an unauthorized import - Consider passing said import under </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">`additional_authorized_imports` when initializing your CodeAgent.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mWarning to user: Code execution failed due to an unauthorized import - Consider passing said import under \u001b[0m\n",
       "\u001b[1;31m`additional_authorized_imports` when initializing your CodeAgent.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Code execution failed at line 'import os' due to: InterpreterError: Import of os is not allowed. Authorized imports</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">are: ['re', 'stat', 'unicodedata', 'math', 'random', 'itertools', 'collections', 'queue', 'statistics', 'datetime',</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">'time'\\]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mCode execution failed at line 'import os' due to: InterpreterError: Import of os is not allowed. Authorized imports\u001b[0m\n",
       "\u001b[1;31mare: ['re', 'stat', 'unicodedata', 'math', 'random', 'itertools', 'collections', 'queue', 'statistics', 'datetime',\u001b[0m\n",
       "\u001b[1;31m'time'\\]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.55 seconds| Input tokens: 7,275 | Output tokens: 342]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.55 seconds| Input tokens: 7,275 | Output tokens: 342]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Let's try to filter functions from the main module we're working with</span><span style=\"background-color: #272822\">                                        </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Since we don't know which specific module contains the functions we're looking for,</span><span style=\"background-color: #272822\">                          </span>  \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># let me try filtering for functions generally</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">functions </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> filter_symbols(kind</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'function'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, max_results</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">100</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, max_chars</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">100</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Functions found:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, functions[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'result'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">])</span><span style=\"background-color: #272822\">                                                                 </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Let's try to filter functions from the main module we're working with\u001b[0m\u001b[48;2;39;40;34m                                        \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Since we don't know which specific module contains the functions we're looking for,\u001b[0m\u001b[48;2;39;40;34m                          \u001b[0m  \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# let me try filtering for functions generally\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfunctions\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfilter_symbols\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkind\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mfunction\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmax_results\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m100\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmax_chars\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m100\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mFunctions found:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunctions\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresult\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                 \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Functions found: [{'name': 'format_dialog', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/dataset.py'}, {'name': 'build_dataset', 'kind': 'function', 'type': \n",
       "None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/dataset.py'}, {'name': '__init__', 'kind': \n",
       "'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': \n",
       "'_merge', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': '_token_id_mapping', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': '_split_data', \n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, \n",
       "{'name': 'train', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': '_get_byte_encoder', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': \n",
       "'_get_byte_decoder', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'save', 'kind': 'function', 'type': None, \n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'load', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'encode', 'kind': \n",
       "'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': \n",
       "'decode', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'tokenize', 'kind': 'function', 'type': None,\n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'clean_mem', 'kind': 'function',\n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'create_logger', \n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': \n",
       "'get_ram_usage', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'free_vars', 'kind': 'function', 'type': None, \n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'summary', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'get_precision_bytes',\n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': \n",
       "'get_max_depth', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'compute_effective_depth', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'align_columns', \n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': \n",
       "'pretty_size', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'count_params', 'kind': 'function', 'type': None,\n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'estimate_memory_footprint', 'kind':\n",
       "'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': \n",
       "'get_module_info', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'hook_fn', 'kind': 'function', 'type': None, \n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': '_print', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'capture_print', \n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}]\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Functions found: [{'name': 'format_dialog', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/dataset.py'}, {'name': 'build_dataset', 'kind': 'function', 'type': \n",
       "None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/dataset.py'}, {'name': '__init__', 'kind': \n",
       "'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': \n",
       "'_merge', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': '_token_id_mapping', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': '_split_data', \n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, \n",
       "{'name': 'train', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': '_get_byte_encoder', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': \n",
       "'_get_byte_decoder', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'save', 'kind': 'function', 'type': None, \n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'load', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'encode', 'kind': \n",
       "'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': \n",
       "'decode', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'tokenize', 'kind': 'function', 'type': None,\n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py'}, {'name': 'clean_mem', 'kind': 'function',\n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'create_logger', \n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': \n",
       "'get_ram_usage', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'free_vars', 'kind': 'function', 'type': None, \n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'summary', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'get_precision_bytes',\n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': \n",
       "'get_max_depth', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'compute_effective_depth', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'align_columns', \n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': \n",
       "'pretty_size', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'count_params', 'kind': 'function', 'type': None,\n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'estimate_memory_footprint', 'kind':\n",
       "'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': \n",
       "'get_module_info', 'kind': 'function', 'type': None, 'file_path': \n",
       "'/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'hook_fn', 'kind': 'function', 'type': None, \n",
       "'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': '_print', 'kind': 'function', \n",
       "'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}, {'name': 'capture_print', \n",
       "'kind': 'function', 'type': None, 'file_path': '/home/rudy/code/karpathy-gpt/src/medievallm/utils.py'}]\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 3.40 seconds| Input tokens: 11,581 | Output tokens: 516]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 3.40 seconds| Input tokens: 11,581 | Output tokens: 516]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 4</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 4\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Extract just the function names from the results</span><span style=\"background-color: #272822\">                                                             </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">function_names </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> [func[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'name'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">] </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> func </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> functions[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'result'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]]</span><span style=\"background-color: #272822\">                                                </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"Function names only:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                                                  </span>  \n",
       "  <span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> name </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> function_names:</span><span style=\"background-color: #272822\">                                                                                    </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">    print(name)</span><span style=\"background-color: #272822\">                                                                                                </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;149;144;119;48;2;39;40;34m# Extract just the function names from the results\u001b[0m\u001b[48;2;39;40;34m                                                             \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfunction_names\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunc\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mname\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunc\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunctions\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresult\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[48;2;39;40;34m                                                \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mFunction names only:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                  \u001b[0m  \n",
       "  \u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mname\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunction_names\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m:\u001b[0m\u001b[48;2;39;40;34m                                                                                    \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34m    \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mname\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                                \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "Function names only:\n",
       "format_dialog\n",
       "build_dataset\n",
       "__init__\n",
       "_merge\n",
       "_token_id_mapping\n",
       "_split_data\n",
       "train\n",
       "_get_byte_encoder\n",
       "_get_byte_decoder\n",
       "save\n",
       "load\n",
       "encode\n",
       "decode\n",
       "tokenize\n",
       "clean_mem\n",
       "create_logger\n",
       "get_ram_usage\n",
       "free_vars\n",
       "summary\n",
       "get_precision_bytes\n",
       "get_max_depth\n",
       "compute_effective_depth\n",
       "align_columns\n",
       "pretty_size\n",
       "count_params\n",
       "estimate_memory_footprint\n",
       "get_module_info\n",
       "hook_fn\n",
       "_print\n",
       "capture_print\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "Function names only:\n",
       "format_dialog\n",
       "build_dataset\n",
       "__init__\n",
       "_merge\n",
       "_token_id_mapping\n",
       "_split_data\n",
       "train\n",
       "_get_byte_encoder\n",
       "_get_byte_decoder\n",
       "save\n",
       "load\n",
       "encode\n",
       "decode\n",
       "tokenize\n",
       "clean_mem\n",
       "create_logger\n",
       "get_ram_usage\n",
       "free_vars\n",
       "summary\n",
       "get_precision_bytes\n",
       "get_max_depth\n",
       "compute_effective_depth\n",
       "align_columns\n",
       "pretty_size\n",
       "count_params\n",
       "estimate_memory_footprint\n",
       "get_module_info\n",
       "hook_fn\n",
       "_print\n",
       "capture_print\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 4: Duration 1.83 seconds| Input tokens: 17,479 | Output tokens: 593]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 4: Duration 1.83 seconds| Input tokens: 17,479 | Output tokens: 593]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 5</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 5\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer([func[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'name'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">] </span><span style=\"color: #66d9ef; text-decoration-color: #66d9ef; background-color: #272822\">for</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> func </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">in</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> functions[</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">'result'</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">]])</span><span style=\"background-color: #272822\">                                                   </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunc\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mname\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;102;217;239;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunc\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfunctions\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m[\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mresult\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m'\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m]\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                   \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: ['format_dialog', 'build_dataset', '__init__', '_merge', '_token_id_mapping', '_split_data', 'train',</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">'_get_byte_encoder', '_get_byte_decoder', 'save', 'load', 'encode', 'decode', 'tokenize', 'clean_mem', </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">'create_logger', 'get_ram_usage', 'free_vars', 'summary', 'get_precision_bytes', 'get_max_depth', </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">'compute_effective_depth', 'align_columns', 'pretty_size', 'count_params', 'estimate_memory_footprint', </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">'get_module_info', 'hook_fn', '_print', 'capture_print']</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: ['format_dialog', 'build_dataset', '__init__', '_merge', '_token_id_mapping', '_split_data', 'train',\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m'_get_byte_encoder', '_get_byte_decoder', 'save', 'load', 'encode', 'decode', 'tokenize', 'clean_mem', \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m'create_logger', 'get_ram_usage', 'free_vars', 'summary', 'get_precision_bytes', 'get_max_depth', \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m'compute_effective_depth', 'align_columns', 'pretty_size', 'count_params', 'estimate_memory_footprint', \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m'get_module_info', 'hook_fn', '_print', 'capture_print']\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 5: Duration 1.02 seconds| Input tokens: 23,658 | Output tokens: 613]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 5: Duration 1.02 seconds| Input tokens: 23,658 | Output tokens: 613]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with MCPClient(params, structured_output=True) as tools:\n",
    "    agent = CodeAgent(tools=tools, model=model)\n",
    "    agent.run(\n",
    "        \"Give me a list of the functions defined in this project. I want only function names and do not require metadata. Use the filter tool.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "950972e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_parameters = StdioServerParameters(\n",
    "    command=MCP_BIN,  # Using uvx ensures dependencies are available\n",
    "    args=[\"--workspace\", WORKSPACE, \"--lsp\", LSP_BIN, \"--\", \"--stdio\"],\n",
    "    env=os.environ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b680d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InferenceClientModel(\n",
    "    \"Qwen/Qwen3-Coder-30B-A3B-Instruct\",\n",
    "    provider=\"nebius\",\n",
    "    api_key=os.environ[\"NEBIUS_API_KEY\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18437f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9076/2224094001.py:1: FutureWarning: Parameter 'structured_output' was not specified. Currently it defaults to False, but in version 1.25, the default will change to True. To suppress this warning, explicitly set structured_output=True (new behavior) or structured_output=False (legacy behavior). See documentation at https://huggingface.co/docs/smolagents/tutorials/tools#structured-output-and-output-schema-support for more details.\n",
      "  with MCPClient(server_parameters) as tools:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">What parameters does MedievalLMTokenizer accept?</span>                                                                <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - Qwen/Qwen3-Coder-30B-A3B-Instruct ──────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mWhat parameters does MedievalLMTokenizer accept?\u001b[0m                                                                \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - Qwen/Qwen3-Coder-30B-A3B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'definition' with arguments: {'symbolName': 'MedievalLMTokenizer'}                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'definition' with arguments: {'symbolName': 'MedievalLMTokenizer'}                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: ---\n",
       "\n",
       "Symbol: MedievalLMTokenizer\n",
       "File: /home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py\n",
       "Kind: Class\n",
       "Range: L13:C1 - L531:C41\n",
       "\n",
       " 13|class MedievalLMTokenizer:\n",
       " 14|    \"\"\"\n",
       " 15|    Minimal refactor of the provided BPE training snippet into a class.\n",
       " 16|    Preserves the original algorithm/logic exactly while exposing configuration.\n",
       " 17|\n",
       " 18|    Usage:\n",
       " 19|        trainer = MedievalLMTokenizer(text)\n",
       " 20|        vocab, vocab_to_str, merged_pairs = trainer.train()\n",
       " 21|    \"\"\"\n",
       " 22|\n",
       " 23|    def __init__(\n",
       " 24|        self,\n",
       " 25|        min_merge_threshold: int = 400,\n",
       " 26|        max_token_threshold: int = 512,\n",
       " 27|        splitting_pattern: Optional|re.Pattern] = None,\n",
       " 28|        special_tokens: Optional|List|str]] = None,\n",
       " 29|        text: str = None,\n",
       " 30|        base_path: str = None,\n",
       " 31|        verbose: int = 1,\n",
       " 32|        logger: logging.Logger = None,\n",
       " 33|    ):\n",
       " 34|        \"\"\"\n",
       " 35|        Parameters mirror the original snippet, defaults set to your example.\n",
       " 36|        - text: input text to train BPE on\n",
       " 37|        - min_merge_threshold: minimum pair frequency to continue merging\n",
       " 38|        - max_token_threshold: target vocab size (stop when vocab reaches this)\n",
       " 39|        - splitting_pattern: compiled regex pattern to split text into subwords\n",
       " 40|        - special_tokens: optional list of special tokens to append at the end\n",
       " 41|        - base_path: optional path to load the tokenizer from\n",
       " 42|        - verbose: if True prints the same status lines as the original snippet\n",
       " 43|        \"\"\"\n",
       " 44|        self.min_merge_threshold = min_merge_threshold\n",
       " 45|        self.max_token_threshold = max_token_threshold\n",
       " 46|        self.verbose = verbose\n",
       " 47|        self.base_path = base_path\n",
       " 48|        if verbose:\n",
       " 49|            if logger:\n",
       " 50|                self.print_func = logger.info\n",
       " 51|            else:\n",
       " 52|                self.print_func = print\n",
       " 53|\n",
       " 54|        # Default splitting pattern is exactly as in the snippet\n",
       " 55|        if splitting_pattern is None:\n",
       " 56|            # NOTE: kept exactly the same raw pattern string as provided\n",
       " 57|            self.splitting_pattern = re.compile(\n",
       " 58|                r\"\"\"'|a-zA-Z]+| ?\\p{L}+| ?\\p{N}+| ?|^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)\"\"\"\n",
       " 59|            )\n",
       " 60|        else:\n",
       " 61|            self.splitting_pattern = splitting_pattern\n",
       " 62|\n",
       " 63|        if special_tokens is None:\n",
       " 64|            self.unk_token = \"&lt;UNK&gt;\"  # Unknown token\n",
       " 65|            self.pad_token = \"&lt;PAD&gt;\"  # Padding token\n",
       " 66|            self.bos_token = \"&lt;BOS&gt;\"  # Beginning of sequence\n",
       " 67|            self.eos_token = \"&lt;EOS&gt;\"  # End of sequence\n",
       " 68|            self.modern_token = \"&lt;USER&gt;\"  # Modern English input\n",
       " 69|            self.shakespeare_token = \"&lt;SHAKESPEARE&gt;\"  # Shakespearean output\n",
       " 70|            self.speaker_start_token = \"&lt;SPEAKER&gt;\"  # Character speaking\n",
       " 71|            self.speaker_end_token = \"&lt;/SPEAKER&gt;\"  # Character speaking\n",
       " 72|            self.dialog_start_token = \"&lt;DIALOG&gt;\"  # Stage directions\n",
       " 73|            self.dialog_end_token = \"&lt;/DIALOG&gt;\"  # Stage directions\n",
       " 74|\n",
       " 75|            self.special_tokens = |\n",
       " 76|                self.unk_token,\n",
       " 77|                self.pad_token,\n",
       " 78|                self.bos_token,\n",
       " 79|                self.eos_token,\n",
       " 80|                self.modern_token,\n",
       " 81|                self.shakespeare_token,\n",
       " 82|                self.speaker_start_token,\n",
       " 83|                self.speaker_end_token,\n",
       " 84|                self.dialog_start_token,\n",
       " 85|                self.dialog_end_token,\n",
       " 86|            ]\n",
       " 87|        else:\n",
       " 88|            self.special_tokens = special_tokens\n",
       " 89|\n",
       " 90|        # Internal state that training will populate\n",
       " 91|        self.vocab: List|int] = |]\n",
       " 92|        self.vocab_to_str: Dict|int, str] = {}\n",
       " 93|        self.merged_pairs: Dict|Tuple|int, int], int] = {}\n",
       " 94|\n",
       " 95|        if text is not None:\n",
       " 96|            _ = self.train(text, base_path=self.base_path)\n",
       " 97|        if base_path is not None:\n",
       " 98|            _ = self.load(base_path=self.base_path)\n",
       " 99|\n",
       "100|    def _merge(\n",
       "101|        self,\n",
       "102|        subwords_tokens: List|List|int]],\n",
       "103|        most_frequent_pair: Tuple|int, int],\n",
       "104|        new_token: int,\n",
       "105|        pair_counts: Optional|Dict|Tuple|int, int], int]] = None,\n",
       "106|    ) -&gt; Tuple|List|List|int]], Dict|Tuple|int, int], int]]:\n",
       "107|        \"\"\"\n",
       "108|        Merge occurrences of `most_frequent_pair` in subwords_tokens into `new_token`.\n",
       "109|\n",
       "110|        If pair_counts is not provided, a defaultdict(int) is created and initialized\n",
       "111|        with counts of all adjacent pairs in `subwords_tokens`.\n",
       "112|\n",
       "113|        The function updates pair_counts in-place to reflect the removals/additions\n",
       "114|        of adjacent pairs caused by each merge and returns (merged_subwords_tokens, pair_counts).\n",
       "115|\n",
       "116|        Args:\n",
       "117|            subwords_tokens: list of tokenized subword sequences (lists of ints)\n",
       "118|            most_frequent_pair: the pair (a, b) to merge\n",
       "119|            new_token: integer id to use for the merged token\n",
       "120|            pair_counts: optional dict mapping (token_i, token_j) -&gt; count\n",
       "121|\n",
       "122|        Returns:\n",
       "123|            (merged_subwords_tokens, pair_counts)\n",
       "124|        \"\"\"\n",
       "125|        if pair_counts is None:\n",
       "126|            pair_counts = defaultdict(int)\n",
       "127|        a, b = most_frequent_pair\n",
       "128|        merged_subwords_tokens: List|List|int]] = |]\n",
       "129|        for subsequence in subwords_tokens:\n",
       "130|            # short-circuit for very short sequences\n",
       "131|            if len(subsequence) &lt; 2:\n",
       "132|                # copy to avoid mutating input lists\n",
       "133|                merged_subwords_tokens.append(list(subsequence))\n",
       "134|                continue\n",
       "135|\n",
       "136|            merged_subsequence: List|int] = |]\n",
       "137|            i = 0\n",
       "138|            L = len(subsequence)\n",
       "139|\n",
       "140|            while i &lt; L:\n",
       "141|                # If we can merge at this position\n",
       "142|                if (i &lt;= L - 2) and ((subsequence|i], subsequence|i + 1]) == (a, b)):\n",
       "143|                    # neighbors in the original subsequence (before this merge)\n",
       "144|                    prev_token = merged_subsequence|-1] if i - 1 &gt;= 0 else None\n",
       "145|                    next_token = subsequence|i + 2] if i + 2 &lt; L else None\n",
       "146|\n",
       "147|                    # decrement counts for pairs that will disappear:\n",
       "148|                    # (prev, a), (a, b), (b, next)\n",
       "149|                    if prev_token is not None:\n",
       "150|                        pair_counts|(prev_token, subsequence|i])] -= 1\n",
       "151|                    pair_counts|(subsequence|i], subsequence|i + 1])] -= 1\n",
       "152|                    if next_token is not None:\n",
       "153|                        pair_counts|(subsequence|i + 1], next_token)] -= 1\n",
       "154|\n",
       "155|                    # increment counts for newly created pairs: (prev, new_token), (new_token, next)\n",
       "156|                    if prev_token is not None:\n",
       "157|                        pair_counts|(prev_token, new_token)] += 1\n",
       "158|                    if next_token is not None:\n",
       "159|                        pair_counts|(new_token, next_token)] += 1\n",
       "160|\n",
       "161|                    # append the merged token\n",
       "162|                    merged_subsequence.append(new_token)\n",
       "163|                    i += 2  # skip the merged pair\n",
       "164|                else:\n",
       "165|                    # no merge here; append the current token\n",
       "166|                    # but we should be careful to keep counts consistent:\n",
       "167|                    # when we move past a single token not merged, nothing changes to pair counts\n",
       "168|                    merged_subsequence.append(subsequence|i])\n",
       "169|                    i += 1\n",
       "170|\n",
       "171|            merged_subwords_tokens.append(merged_subsequence)\n",
       "172|\n",
       "173|        return merged_subwords_tokens, pair_counts\n",
       "174|\n",
       "175|    def _token_id_mapping(self):\n",
       "176|        self.id_to_token = self.vocab_to_str\n",
       "177|        self.token_to_id = {v: k for k, v in self.id_to_token.items()}\n",
       "178|\n",
       "179|    def _split_data(self, text):\n",
       "180|        subwords = re.findall(self.splitting_pattern, text)\n",
       "181|        subwords_tokens = |list(word.encode(\"utf-8\")) for word in subwords]\n",
       "182|        if self.verbose &gt;= 2:\n",
       "183|            self.print_func(f\"After splitting:\")\n",
       "184|            self.print_func(f\"{len(subwords_tokens) = }\")\n",
       "185|            for i in range(min(len(subwords), 5)):\n",
       "186|                self.print_func(f\"{subwords|i]} -&gt; {subwords_tokens|i]}\")\n",
       "187|        return subwords_tokens\n",
       "188|\n",
       "189|    def train(self, text: str, base_path: str = \".\"):\n",
       "190|        \"\"\"\n",
       "191|        Run the BPE training loop and return (vocab, vocab_to_str, merged_pairs)\n",
       "192|        Preserves printing and logic of the original snippet.\n",
       "193|        \"\"\"\n",
       "194|        # split the text into subwords\n",
       "195|        subwords_tokens = self._split_data(text)\n",
       "196|\n",
       "197|        # get the tokens and the vocab\n",
       "198|        all_tokens = |token for subword in subwords_tokens for token in subword]\n",
       "199|        self.vocab = list(range(256))\n",
       "200|        self.vocab_to_str = {v: chr(v) for i, v in enumerate(self.vocab)}\n",
       "201|\n",
       "202|        merged_pairs = {}\n",
       "203|\n",
       "204|        if self.verbose:\n",
       "205|            all_tokens = |token for subword in subwords_tokens for token in subword]\n",
       "206|            self.print_func(f\"Initial vocab stats:\")\n",
       "207|            self.print_func(f\"  Unique bytes in text: {len(self.vocab)}\")\n",
       "208|            self.print_func(f\"  Total tokens: {len(all_tokens)}\")\n",
       "209|            self.print_func(f\"  Subwords: {len(subwords_tokens)}\")\n",
       "210|            self.print_func(f\"  Target vocab size: {self.max_token_threshold}\")\n",
       "211|            self.print_func(f\"  Min merge threshold: {self.min_merge_threshold}\")\n",
       "212|            self.print_func(\"Starting BPE merging...\")\n",
       "213|            self.print_func(\"Pair\\t\\t\\tFreq\\tNew Token\\tToken String\\t\\tVocab Size\")\n",
       "214|            self.print_func(\"-\" * 80)\n",
       "215|\n",
       "216|        next_token = max(self.vocab) + 1 if self.vocab else 0\n",
       "217|        self.verbose and self.print_func(f\"{next_token = }\")\n",
       "218|        pair_counts = Counter()\n",
       "219|        for tokens in subwords_tokens:\n",
       "220|            for i in range(len(tokens) - 1):\n",
       "221|                pair = (tokens|i], tokens|i + 1])\n",
       "222|                pair_counts|pair] += 1\n",
       "223|        while len(self.vocab) &lt; self.max_token_threshold:\n",
       "224|            if not pair_counts:\n",
       "225|                break\n",
       "226|            most_frequent_pair, actual_count = pair_counts.most_common(1)|0]\n",
       "227|            if actual_count &lt;= self.min_merge_threshold:\n",
       "228|                break\n",
       "229|            subwords_tokens, pair_counts = self._merge(\n",
       "230|                subwords_tokens, most_frequent_pair, next_token, pair_counts\n",
       "231|            )\n",
       "232|            new_token_string = (\n",
       "233|                self.vocab_to_str|most_frequent_pair|0]]\n",
       "234|                + self.vocab_to_str|most_frequent_pair|1]]\n",
       "235|            )\n",
       "236|            self.vocab.append(next_token)\n",
       "237|            self.vocab_to_str|next_token] = new_token_string\n",
       "238|            merged_pairs|most_frequent_pair] = next_token\n",
       "239|            if self.verbose:\n",
       "240|                self.print_func(\n",
       "241|                    \n",
       "f\"{most_frequent_pair}\\t\\t{actual_count}\\t{next_token}\\t\\t'{new_token_string}'\\t\\t\\t{len(self.vocab)}\"\n",
       "242|                )\n",
       "243|            next_token += 1\n",
       "244|        special_token_to_str = {\n",
       "245|            i + max(self.vocab) + 1: v for i, v in enumerate(self.special_tokens)\n",
       "246|        }\n",
       "247|        self.vocab_to_str.update(special_token_to_str)\n",
       "248|        self.vocab.extend(list(special_token_to_str.keys()))\n",
       "249|\n",
       "250|        if self.verbose:\n",
       "251|            self.print_func(\"-\" * 80)\n",
       "252|            self.print_func(\n",
       "253|                f\"BPE training completed. Final vocab size: {len(self.vocab)}\"\n",
       "254|            )\n",
       "255|\n",
       "256|        # Persist state back to object\n",
       "257|        self.merged_pairs = merged_pairs\n",
       "258|\n",
       "259|        if self.base_path is not None:\n",
       "260|            self.save(self.base_path, self.vocab, self.vocab_to_str, self.merged_pairs)\n",
       "261|\n",
       "262|        return self.vocab, self.vocab_to_str, merged_pairs\n",
       "263|\n",
       "264|    def _get_byte_encoder(self):\n",
       "265|        \"\"\"GPT-2 style byte-to-unicode mapping for safely representing bytes as strings\"\"\"\n",
       "266|        bs = (\n",
       "267|            list(range(ord(\"!\"), ord(\"~\") + 1))\n",
       "268|            + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
       "269|            + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
       "270|        )\n",
       "271|        cs = bs|:]\n",
       "272|        n = 0\n",
       "273|        for b in range(2**8):\n",
       "274|            if b not in bs:\n",
       "275|                bs.append(b)\n",
       "276|                cs.append(2**8 + n)\n",
       "277|                n += 1\n",
       "278|        cs = |chr(n) for n in cs]\n",
       "279|        return dict(zip(bs, cs))\n",
       "280|\n",
       "281|    def _get_byte_decoder(self):\n",
       "282|        byte_encoder = self._get_byte_encoder()\n",
       "283|        return {v: k for k, v in byte_encoder.items()}\n",
       "284|\n",
       "285|    def save(\n",
       "286|        self,\n",
       "287|        base_path: str,\n",
       "288|        vocab: list | set = None,\n",
       "289|        vocab_to_str: dict = None,\n",
       "290|        merged_pairs: dict | list = None,\n",
       "291|        version: str = None,\n",
       "292|    ):\n",
       "293|        if vocab is None:\n",
       "294|            vocab = self.vocab\n",
       "295|        if vocab_to_str is None:\n",
       "296|            vocab_to_str = self.vocab_to_str\n",
       "297|        if merged_pairs is None:\n",
       "298|            merged_pairs = self.merged_pairs\n",
       "299|        if version is None:\n",
       "300|            version = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
       "301|\n",
       "302|        if self.verbose:\n",
       "303|            self.print_func(f\"|save] Starting save() -&gt; base_path: {base_path}\")\n",
       "304|            self.print_func(\n",
       "305|                f\"|save] Vocab entries: {len(vocab)}; Merged pairs: {len(merged_pairs)}\"\n",
       "306|            )\n",
       "307|\n",
       "308|        byte_encoder = self._get_byte_encoder()\n",
       "309|\n",
       "310|        if self.verbose:\n",
       "311|            self.print_func(\"|save] Converting vocab_to_str using byte encoder...\")\n",
       "312|\n",
       "313|        # Convert vocab_to_str using byte encoding\n",
       "314|        encoded_vocab_to_str = {}\n",
       "315|        for idx, (token_id, token_str) in enumerate(vocab_to_str.items()):\n",
       "316|            # Convert string to bytes then to encoded representation\n",
       "317|            token_bytes = token_str.encode(\"utf-8\")\n",
       "318|            encoded_str = \"\".join(|byte_encoder|b] for b in token_bytes])\n",
       "319|            encoded_vocab_to_str|token_id] = encoded_str\n",
       "320|\n",
       "321|            # Print a few examples as progress (first 5) and then occasional progress\n",
       "322|            if self.verbose:\n",
       "323|                if idx &lt; 5:\n",
       "324|                    self.print_func(\n",
       "325|                        f\"|save] Encoded example {idx + 1}: token_id={token_id} -&gt; {encoded_str!r}\"\n",
       "326|                    )\n",
       "327|                elif (idx + 1) % 1000 == 0:\n",
       "328|                    self.print_func(f\"|save] Encoded {idx + 1} vocab entries...\")\n",
       "329|\n",
       "330|        if self.verbose:\n",
       "331|            self.print_func(\n",
       "332|                f\"|save] Finished encoding {len(encoded_vocab_to_str)} vocab entries.\"\n",
       "333|            )\n",
       "334|\n",
       "335|        # Save vocab.json (encoded_string -&gt; token_id)\n",
       "336|        encoder = {encoded_vocab_to_str|token_id]: token_id for token_id in vocab}\n",
       "337|        vocab_path = f\"{base_path}_vocab.json\"\n",
       "338|        if self.verbose:\n",
       "339|            self.print_func(\n",
       "340|                f\"|save] Writing vocab to {vocab_path} (encoded_string -&gt; token_id)...\"\n",
       "341|            )\n",
       "342|        with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
       "343|            json.dump(encoder, f, ensure_ascii=False, indent=2)\n",
       "344|        if self.verbose:\n",
       "345|            self.print_func(\n",
       "346|                f\"|save] Wrote vocab to {vocab_path} (entries: {len(encoder)}).\"\n",
       "347|            )\n",
       "348|\n",
       "349|        # Save merges.txt using encoded strings\n",
       "350|        merges_path = f\"{base_path}_merges.txt\"\n",
       "351|        if self.verbose:\n",
       "352|            self.print_func(\n",
       "353|                f\"|save] Writing merges to {merges_path} (human-readable encoded pairs)...\"\n",
       "354|            )\n",
       "355|        with open(merges_path, \"w\", encoding=\"utf-8\") as f:\n",
       "356|            f.write(f\"#version: {version}\\n\")\n",
       "357|            for i, (pair, token_id) in enumerate(merged_pairs.items()):\n",
       "358|                token1_encoded = encoded_vocab_to_str|pair|0]]\n",
       "359|                token2_encoded = encoded_vocab_to_str|pair|1]]\n",
       "360|                f.write(f\"{token1_encoded} {token2_encoded}\\n\")\n",
       "361|\n",
       "362|                # small progress prints for merges\n",
       "363|                if self.verbose and i &lt; 5:\n",
       "364|                    self.print_func(\n",
       "365|                        f\"|save] Merge example {i + 1}: {token1_encoded} + {token2_encoded} -&gt; \n",
       "token_id={token_id}\"\n",
       "366|                    )\n",
       "367|\n",
       "368|        if self.verbose:\n",
       "369|            self.print_func(\n",
       "370|                f\"|save] Wrote merges to {merges_path} (entries: {len(merged_pairs)}).\"\n",
       "371|            )\n",
       "372|            self.print_func(\"|save] Completed save().\")\n",
       "373|\n",
       "374|    def load(self, base_path: str):\n",
       "375|        if self.verbose:\n",
       "376|            self.print_func(f\"|load] Starting load() -&gt; base_path: {base_path}\")\n",
       "377|\n",
       "378|        byte_decoder = self._get_byte_decoder()\n",
       "379|\n",
       "380|        # Load vocab.json\n",
       "381|        vocab_path = f\"{base_path}_vocab.json\"\n",
       "382|        if self.verbose:\n",
       "383|            self.print_func(f\"|load] Reading vocab file: {vocab_path}\")\n",
       "384|        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
       "385|            encoder = json.load(f)\n",
       "386|        if self.verbose:\n",
       "387|            self.print_func(f\"|load] Loaded encoder with {len(encoder)} entries.\")\n",
       "388|\n",
       "389|        # Decode back to original strings\n",
       "390|        vocab_to_str = {}\n",
       "391|        for idx, (encoded_str, token_id) in enumerate(encoder.items()):\n",
       "392|            # Decode from encoded representation back to original string\n",
       "393|            decoded_bytes = bytes(|byte_decoder|c] for c in encoded_str])\n",
       "394|            original_str = decoded_bytes.decode(\"utf-8\")\n",
       "395|            vocab_to_str|token_id] = original_str\n",
       "396|\n",
       "397|            if self.verbose:\n",
       "398|                if idx &lt; 5:\n",
       "399|                    self.print_func(\n",
       "400|                        f\"|load] Decoded example {idx+1}: token_id={token_id} -&gt; {original_str!r}\"\n",
       "401|                    )\n",
       "402|                elif (idx + 1) % 1000 == 0:\n",
       "403|                    self.print_func(f\"|load] Decoded {idx+1} vocab entries...\")\n",
       "404|\n",
       "405|        vocab = sorted(vocab_to_str.keys())\n",
       "406|        if self.verbose:\n",
       "407|            self.print_func(f\"|load] Finished decoding vocab (size={len(vocab)}).\")\n",
       "408|\n",
       "409|        # Load merges.txt\n",
       "410|        merges_path = f\"{base_path}_merges.txt\"\n",
       "411|        if self.verbose:\n",
       "412|            self.print_func(f\"|load] Reading merges file: {merges_path}\")\n",
       "413|        merged_pairs = {}\n",
       "414|        next_token = max(vocab) + 1\n",
       "415|\n",
       "416|        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
       "417|            lines = f.readlines()|1:]  # Skip header\n",
       "418|            for i, line in enumerate(lines):\n",
       "419|                if line.strip():\n",
       "420|                    token1_str, token2_str = line.strip().split(\" \", 1)\n",
       "421|                    # Find token IDs for the encoded strings\n",
       "422|                    token1_id = encoder|token1_str]\n",
       "423|                    token2_id = encoder|token2_str]\n",
       "424|                    merged_pairs|(token1_id, token2_id)] = encoder|\n",
       "425|                        token1_str + token2_str\n",
       "426|                    ]\n",
       "427|                    next_token += 1\n",
       "428|\n",
       "429|                    if self.verbose and i &lt; 5:\n",
       "430|                        self.print_func(\n",
       "431|                            f\"|load] Merge example {i+1}: {token1_str}+{token2_str} \"\n",
       "432|                            f\"-&gt; token_id={merged_pairs|(token1_id, token2_id)]}\"\n",
       "433|                        )\n",
       "434|\n",
       "435|        if self.verbose:\n",
       "436|            self.print_func(f\"|load] Loaded {len(merged_pairs)} merge rules.\")\n",
       "437|            self.print_func(\"|load] Completed load().\")\n",
       "438|\n",
       "439|        self.vocab = vocab\n",
       "440|        self.vocab_to_str = vocab_to_str\n",
       "441|        self.merged_pairs = merged_pairs\n",
       "442|        self._token_id_mapping()\n",
       "443|\n",
       "444|        return vocab, vocab_to_str, merged_pairs\n",
       "445|\n",
       "446|    def encode(\n",
       "447|        self,\n",
       "448|        text: str,\n",
       "449|        normalise: bool = False,\n",
       "450|    ) -&gt; List|int]:\n",
       "451|        \"\"\"\n",
       "452|        Encode text to BPE tokens using learned merged pairs.\n",
       "453|        Args:\n",
       "454|            text: Input string\n",
       "455|            merged_pairs: Dict mapping pairs to new tokens\n",
       "456|            splitting_pattern: Compiled regex splitting pattern\n",
       "457|            vocab_to_str: Dict mapping token id to string representation\n",
       "458|        Returns:\n",
       "459|            List of int tokens\n",
       "460|        \"\"\"\n",
       "461|\n",
       "462|        # Create reverse mapping for special tokens (only tokens starting with &lt; and ending with &gt;)\n",
       "463|        special_token_to_id = {\n",
       "464|            token_str: token_id\n",
       "465|            for token_id, token_str in self.vocab_to_str.items()\n",
       "466|            if token_str in self.special_tokens\n",
       "467|        }\n",
       "468|        sorted_pairs = self.merged_pairs.items()\n",
       "469|\n",
       "470|        # handle special tokens by making a regex pattern to escape them\n",
       "471|        special_patterns = |re.escape(token) for token in special_token_to_id.keys()]\n",
       "472|        special_regex = \"(\" + \"|\".join(special_patterns) + \")\"\n",
       "473|        segments = re.split(special_regex, text)\n",
       "474|        self.verbose and self.print_func(f\"{len(segments) = }\")\n",
       "475|        all_tokens = |]\n",
       "476|        for segment in tqdm(segments, position=0):\n",
       "477|            if segment in special_token_to_id:\n",
       "478|                # This segment is a special token - add its ID directly\n",
       "479|                all_tokens.append(special_token_to_id|segment])\n",
       "480|            elif segment:  # Non-empty regular text segment\n",
       "481|                subwords_tokens = self._split_data(segment)\n",
       "482|                for pair, new_token in tqdm(sorted_pairs, position=1, leave=False):\n",
       "483|                    subwords_tokens, _ = self._merge(subwords_tokens, pair, new_token)\n",
       "484|                segment_tokens = |\n",
       "485|                    token for subword in subwords_tokens for token in subword\n",
       "486|                ]\n",
       "487|                all_tokens.extend(segment_tokens)\n",
       "488|        return all_tokens\n",
       "489|\n",
       "490|    def decode(self, tokens: List|int]) -&gt; str:\n",
       "491|        \"\"\"\n",
       "492|        Decode list of BPE tokens back to string\n",
       "493|        Args:\n",
       "494|            tokens: List of token ids\n",
       "495|            vocab_to_str: Dict mapping token id to string\n",
       "496|        Returns:\n",
       "497|            Decoded string\n",
       "498|        \"\"\"\n",
       "499|        # Map each token to its string representation and concatenate\n",
       "500|        return \"\".join(self.vocab_to_str|token] for token in tqdm(tokens))\n",
       "501|\n",
       "502|    def tokenize(\n",
       "503|        self, texts: list|str], padding_direction: str = \"left\", max_length: int = 128\n",
       "504|    ):\n",
       "505|        # since we dont have the vocab number for the special tokens\n",
       "506|        # this is a hacky way to get that\n",
       "507|        # then we can use it in token_to_id\n",
       "508|        special_token_to_id = {\n",
       "509|            token_str: token_id\n",
       "510|            for token_id, token_str in self.vocab_to_str.items()\n",
       "511|            if token_str.startswith(\"&lt;\") and token_str.endswith(\"&gt;\")\n",
       "512|        }\n",
       "513|        pad_id = self.token_to_id|special_token_to_id|self.pad_token]]\n",
       "514|        encoded = |self.encode(t)|:max_length] for t in texts]\n",
       "515|        # pre allocate numpy arrays\n",
       "516|        batch_size = len(encoded)\n",
       "517|        input_ids = np.full((batch_size, max_length), pad_id, dtype=np.int32)\n",
       "518|        attention_mask = np.zeros((batch_size, max_length), dtype=np.int32)\n",
       "519|        # fill in the input_ids and attention_mask\n",
       "520|        for i, seq in enumerate(encoded):\n",
       "521|            length = len(seq)\n",
       "522|            if padding_direction == \"right\":\n",
       "523|                input_ids|i, :length] = seq\n",
       "524|                attention_mask|i, :length] = 1\n",
       "525|            elif padding_direction == \"left\":\n",
       "526|                input_ids|i, -length:] = seq\n",
       "527|                attention_mask|i, -length:] = 1\n",
       "528|            else:\n",
       "529|                raise ValueError(\"padding_direction must be 'left' or 'right'\")\n",
       "530|\n",
       "531|        return input_ids, attention_mask\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: ---\n",
       "\n",
       "Symbol: MedievalLMTokenizer\n",
       "File: /home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py\n",
       "Kind: Class\n",
       "Range: L13:C1 - L531:C41\n",
       "\n",
       " 13|class MedievalLMTokenizer:\n",
       " 14|    \"\"\"\n",
       " 15|    Minimal refactor of the provided BPE training snippet into a class.\n",
       " 16|    Preserves the original algorithm/logic exactly while exposing configuration.\n",
       " 17|\n",
       " 18|    Usage:\n",
       " 19|        trainer = MedievalLMTokenizer(text)\n",
       " 20|        vocab, vocab_to_str, merged_pairs = trainer.train()\n",
       " 21|    \"\"\"\n",
       " 22|\n",
       " 23|    def __init__(\n",
       " 24|        self,\n",
       " 25|        min_merge_threshold: int = 400,\n",
       " 26|        max_token_threshold: int = 512,\n",
       " 27|        splitting_pattern: Optional|re.Pattern] = None,\n",
       " 28|        special_tokens: Optional|List|str]] = None,\n",
       " 29|        text: str = None,\n",
       " 30|        base_path: str = None,\n",
       " 31|        verbose: int = 1,\n",
       " 32|        logger: logging.Logger = None,\n",
       " 33|    ):\n",
       " 34|        \"\"\"\n",
       " 35|        Parameters mirror the original snippet, defaults set to your example.\n",
       " 36|        - text: input text to train BPE on\n",
       " 37|        - min_merge_threshold: minimum pair frequency to continue merging\n",
       " 38|        - max_token_threshold: target vocab size (stop when vocab reaches this)\n",
       " 39|        - splitting_pattern: compiled regex pattern to split text into subwords\n",
       " 40|        - special_tokens: optional list of special tokens to append at the end\n",
       " 41|        - base_path: optional path to load the tokenizer from\n",
       " 42|        - verbose: if True prints the same status lines as the original snippet\n",
       " 43|        \"\"\"\n",
       " 44|        self.min_merge_threshold = min_merge_threshold\n",
       " 45|        self.max_token_threshold = max_token_threshold\n",
       " 46|        self.verbose = verbose\n",
       " 47|        self.base_path = base_path\n",
       " 48|        if verbose:\n",
       " 49|            if logger:\n",
       " 50|                self.print_func = logger.info\n",
       " 51|            else:\n",
       " 52|                self.print_func = print\n",
       " 53|\n",
       " 54|        # Default splitting pattern is exactly as in the snippet\n",
       " 55|        if splitting_pattern is None:\n",
       " 56|            # NOTE: kept exactly the same raw pattern string as provided\n",
       " 57|            self.splitting_pattern = re.compile(\n",
       " 58|                r\"\"\"'|a-zA-Z]+| ?\\p{L}+| ?\\p{N}+| ?|^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)\"\"\"\n",
       " 59|            )\n",
       " 60|        else:\n",
       " 61|            self.splitting_pattern = splitting_pattern\n",
       " 62|\n",
       " 63|        if special_tokens is None:\n",
       " 64|            self.unk_token = \"<UNK>\"  # Unknown token\n",
       " 65|            self.pad_token = \"<PAD>\"  # Padding token\n",
       " 66|            self.bos_token = \"<BOS>\"  # Beginning of sequence\n",
       " 67|            self.eos_token = \"<EOS>\"  # End of sequence\n",
       " 68|            self.modern_token = \"<USER>\"  # Modern English input\n",
       " 69|            self.shakespeare_token = \"<SHAKESPEARE>\"  # Shakespearean output\n",
       " 70|            self.speaker_start_token = \"<SPEAKER>\"  # Character speaking\n",
       " 71|            self.speaker_end_token = \"</SPEAKER>\"  # Character speaking\n",
       " 72|            self.dialog_start_token = \"<DIALOG>\"  # Stage directions\n",
       " 73|            self.dialog_end_token = \"</DIALOG>\"  # Stage directions\n",
       " 74|\n",
       " 75|            self.special_tokens = |\n",
       " 76|                self.unk_token,\n",
       " 77|                self.pad_token,\n",
       " 78|                self.bos_token,\n",
       " 79|                self.eos_token,\n",
       " 80|                self.modern_token,\n",
       " 81|                self.shakespeare_token,\n",
       " 82|                self.speaker_start_token,\n",
       " 83|                self.speaker_end_token,\n",
       " 84|                self.dialog_start_token,\n",
       " 85|                self.dialog_end_token,\n",
       " 86|            ]\n",
       " 87|        else:\n",
       " 88|            self.special_tokens = special_tokens\n",
       " 89|\n",
       " 90|        # Internal state that training will populate\n",
       " 91|        self.vocab: List|int] = |]\n",
       " 92|        self.vocab_to_str: Dict|int, str] = {}\n",
       " 93|        self.merged_pairs: Dict|Tuple|int, int], int] = {}\n",
       " 94|\n",
       " 95|        if text is not None:\n",
       " 96|            _ = self.train(text, base_path=self.base_path)\n",
       " 97|        if base_path is not None:\n",
       " 98|            _ = self.load(base_path=self.base_path)\n",
       " 99|\n",
       "100|    def _merge(\n",
       "101|        self,\n",
       "102|        subwords_tokens: List|List|int]],\n",
       "103|        most_frequent_pair: Tuple|int, int],\n",
       "104|        new_token: int,\n",
       "105|        pair_counts: Optional|Dict|Tuple|int, int], int]] = None,\n",
       "106|    ) -> Tuple|List|List|int]], Dict|Tuple|int, int], int]]:\n",
       "107|        \"\"\"\n",
       "108|        Merge occurrences of `most_frequent_pair` in subwords_tokens into `new_token`.\n",
       "109|\n",
       "110|        If pair_counts is not provided, a defaultdict(int) is created and initialized\n",
       "111|        with counts of all adjacent pairs in `subwords_tokens`.\n",
       "112|\n",
       "113|        The function updates pair_counts in-place to reflect the removals/additions\n",
       "114|        of adjacent pairs caused by each merge and returns (merged_subwords_tokens, pair_counts).\n",
       "115|\n",
       "116|        Args:\n",
       "117|            subwords_tokens: list of tokenized subword sequences (lists of ints)\n",
       "118|            most_frequent_pair: the pair (a, b) to merge\n",
       "119|            new_token: integer id to use for the merged token\n",
       "120|            pair_counts: optional dict mapping (token_i, token_j) -> count\n",
       "121|\n",
       "122|        Returns:\n",
       "123|            (merged_subwords_tokens, pair_counts)\n",
       "124|        \"\"\"\n",
       "125|        if pair_counts is None:\n",
       "126|            pair_counts = defaultdict(int)\n",
       "127|        a, b = most_frequent_pair\n",
       "128|        merged_subwords_tokens: List|List|int]] = |]\n",
       "129|        for subsequence in subwords_tokens:\n",
       "130|            # short-circuit for very short sequences\n",
       "131|            if len(subsequence) < 2:\n",
       "132|                # copy to avoid mutating input lists\n",
       "133|                merged_subwords_tokens.append(list(subsequence))\n",
       "134|                continue\n",
       "135|\n",
       "136|            merged_subsequence: List|int] = |]\n",
       "137|            i = 0\n",
       "138|            L = len(subsequence)\n",
       "139|\n",
       "140|            while i < L:\n",
       "141|                # If we can merge at this position\n",
       "142|                if (i <= L - 2) and ((subsequence|i], subsequence|i + 1]) == (a, b)):\n",
       "143|                    # neighbors in the original subsequence (before this merge)\n",
       "144|                    prev_token = merged_subsequence|-1] if i - 1 >= 0 else None\n",
       "145|                    next_token = subsequence|i + 2] if i + 2 < L else None\n",
       "146|\n",
       "147|                    # decrement counts for pairs that will disappear:\n",
       "148|                    # (prev, a), (a, b), (b, next)\n",
       "149|                    if prev_token is not None:\n",
       "150|                        pair_counts|(prev_token, subsequence|i])] -= 1\n",
       "151|                    pair_counts|(subsequence|i], subsequence|i + 1])] -= 1\n",
       "152|                    if next_token is not None:\n",
       "153|                        pair_counts|(subsequence|i + 1], next_token)] -= 1\n",
       "154|\n",
       "155|                    # increment counts for newly created pairs: (prev, new_token), (new_token, next)\n",
       "156|                    if prev_token is not None:\n",
       "157|                        pair_counts|(prev_token, new_token)] += 1\n",
       "158|                    if next_token is not None:\n",
       "159|                        pair_counts|(new_token, next_token)] += 1\n",
       "160|\n",
       "161|                    # append the merged token\n",
       "162|                    merged_subsequence.append(new_token)\n",
       "163|                    i += 2  # skip the merged pair\n",
       "164|                else:\n",
       "165|                    # no merge here; append the current token\n",
       "166|                    # but we should be careful to keep counts consistent:\n",
       "167|                    # when we move past a single token not merged, nothing changes to pair counts\n",
       "168|                    merged_subsequence.append(subsequence|i])\n",
       "169|                    i += 1\n",
       "170|\n",
       "171|            merged_subwords_tokens.append(merged_subsequence)\n",
       "172|\n",
       "173|        return merged_subwords_tokens, pair_counts\n",
       "174|\n",
       "175|    def _token_id_mapping(self):\n",
       "176|        self.id_to_token = self.vocab_to_str\n",
       "177|        self.token_to_id = {v: k for k, v in self.id_to_token.items()}\n",
       "178|\n",
       "179|    def _split_data(self, text):\n",
       "180|        subwords = re.findall(self.splitting_pattern, text)\n",
       "181|        subwords_tokens = |list(word.encode(\"utf-8\")) for word in subwords]\n",
       "182|        if self.verbose >= 2:\n",
       "183|            self.print_func(f\"After splitting:\")\n",
       "184|            self.print_func(f\"{len(subwords_tokens) = }\")\n",
       "185|            for i in range(min(len(subwords), 5)):\n",
       "186|                self.print_func(f\"{subwords|i]} -> {subwords_tokens|i]}\")\n",
       "187|        return subwords_tokens\n",
       "188|\n",
       "189|    def train(self, text: str, base_path: str = \".\"):\n",
       "190|        \"\"\"\n",
       "191|        Run the BPE training loop and return (vocab, vocab_to_str, merged_pairs)\n",
       "192|        Preserves printing and logic of the original snippet.\n",
       "193|        \"\"\"\n",
       "194|        # split the text into subwords\n",
       "195|        subwords_tokens = self._split_data(text)\n",
       "196|\n",
       "197|        # get the tokens and the vocab\n",
       "198|        all_tokens = |token for subword in subwords_tokens for token in subword]\n",
       "199|        self.vocab = list(range(256))\n",
       "200|        self.vocab_to_str = {v: chr(v) for i, v in enumerate(self.vocab)}\n",
       "201|\n",
       "202|        merged_pairs = {}\n",
       "203|\n",
       "204|        if self.verbose:\n",
       "205|            all_tokens = |token for subword in subwords_tokens for token in subword]\n",
       "206|            self.print_func(f\"Initial vocab stats:\")\n",
       "207|            self.print_func(f\"  Unique bytes in text: {len(self.vocab)}\")\n",
       "208|            self.print_func(f\"  Total tokens: {len(all_tokens)}\")\n",
       "209|            self.print_func(f\"  Subwords: {len(subwords_tokens)}\")\n",
       "210|            self.print_func(f\"  Target vocab size: {self.max_token_threshold}\")\n",
       "211|            self.print_func(f\"  Min merge threshold: {self.min_merge_threshold}\")\n",
       "212|            self.print_func(\"Starting BPE merging...\")\n",
       "213|            self.print_func(\"Pair\\t\\t\\tFreq\\tNew Token\\tToken String\\t\\tVocab Size\")\n",
       "214|            self.print_func(\"-\" * 80)\n",
       "215|\n",
       "216|        next_token = max(self.vocab) + 1 if self.vocab else 0\n",
       "217|        self.verbose and self.print_func(f\"{next_token = }\")\n",
       "218|        pair_counts = Counter()\n",
       "219|        for tokens in subwords_tokens:\n",
       "220|            for i in range(len(tokens) - 1):\n",
       "221|                pair = (tokens|i], tokens|i + 1])\n",
       "222|                pair_counts|pair] += 1\n",
       "223|        while len(self.vocab) < self.max_token_threshold:\n",
       "224|            if not pair_counts:\n",
       "225|                break\n",
       "226|            most_frequent_pair, actual_count = pair_counts.most_common(1)|0]\n",
       "227|            if actual_count <= self.min_merge_threshold:\n",
       "228|                break\n",
       "229|            subwords_tokens, pair_counts = self._merge(\n",
       "230|                subwords_tokens, most_frequent_pair, next_token, pair_counts\n",
       "231|            )\n",
       "232|            new_token_string = (\n",
       "233|                self.vocab_to_str|most_frequent_pair|0]]\n",
       "234|                + self.vocab_to_str|most_frequent_pair|1]]\n",
       "235|            )\n",
       "236|            self.vocab.append(next_token)\n",
       "237|            self.vocab_to_str|next_token] = new_token_string\n",
       "238|            merged_pairs|most_frequent_pair] = next_token\n",
       "239|            if self.verbose:\n",
       "240|                self.print_func(\n",
       "241|                    \n",
       "f\"{most_frequent_pair}\\t\\t{actual_count}\\t{next_token}\\t\\t'{new_token_string}'\\t\\t\\t{len(self.vocab)}\"\n",
       "242|                )\n",
       "243|            next_token += 1\n",
       "244|        special_token_to_str = {\n",
       "245|            i + max(self.vocab) + 1: v for i, v in enumerate(self.special_tokens)\n",
       "246|        }\n",
       "247|        self.vocab_to_str.update(special_token_to_str)\n",
       "248|        self.vocab.extend(list(special_token_to_str.keys()))\n",
       "249|\n",
       "250|        if self.verbose:\n",
       "251|            self.print_func(\"-\" * 80)\n",
       "252|            self.print_func(\n",
       "253|                f\"BPE training completed. Final vocab size: {len(self.vocab)}\"\n",
       "254|            )\n",
       "255|\n",
       "256|        # Persist state back to object\n",
       "257|        self.merged_pairs = merged_pairs\n",
       "258|\n",
       "259|        if self.base_path is not None:\n",
       "260|            self.save(self.base_path, self.vocab, self.vocab_to_str, self.merged_pairs)\n",
       "261|\n",
       "262|        return self.vocab, self.vocab_to_str, merged_pairs\n",
       "263|\n",
       "264|    def _get_byte_encoder(self):\n",
       "265|        \"\"\"GPT-2 style byte-to-unicode mapping for safely representing bytes as strings\"\"\"\n",
       "266|        bs = (\n",
       "267|            list(range(ord(\"!\"), ord(\"~\") + 1))\n",
       "268|            + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
       "269|            + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
       "270|        )\n",
       "271|        cs = bs|:]\n",
       "272|        n = 0\n",
       "273|        for b in range(2**8):\n",
       "274|            if b not in bs:\n",
       "275|                bs.append(b)\n",
       "276|                cs.append(2**8 + n)\n",
       "277|                n += 1\n",
       "278|        cs = |chr(n) for n in cs]\n",
       "279|        return dict(zip(bs, cs))\n",
       "280|\n",
       "281|    def _get_byte_decoder(self):\n",
       "282|        byte_encoder = self._get_byte_encoder()\n",
       "283|        return {v: k for k, v in byte_encoder.items()}\n",
       "284|\n",
       "285|    def save(\n",
       "286|        self,\n",
       "287|        base_path: str,\n",
       "288|        vocab: list | set = None,\n",
       "289|        vocab_to_str: dict = None,\n",
       "290|        merged_pairs: dict | list = None,\n",
       "291|        version: str = None,\n",
       "292|    ):\n",
       "293|        if vocab is None:\n",
       "294|            vocab = self.vocab\n",
       "295|        if vocab_to_str is None:\n",
       "296|            vocab_to_str = self.vocab_to_str\n",
       "297|        if merged_pairs is None:\n",
       "298|            merged_pairs = self.merged_pairs\n",
       "299|        if version is None:\n",
       "300|            version = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
       "301|\n",
       "302|        if self.verbose:\n",
       "303|            self.print_func(f\"|save] Starting save() -> base_path: {base_path}\")\n",
       "304|            self.print_func(\n",
       "305|                f\"|save] Vocab entries: {len(vocab)}; Merged pairs: {len(merged_pairs)}\"\n",
       "306|            )\n",
       "307|\n",
       "308|        byte_encoder = self._get_byte_encoder()\n",
       "309|\n",
       "310|        if self.verbose:\n",
       "311|            self.print_func(\"|save] Converting vocab_to_str using byte encoder...\")\n",
       "312|\n",
       "313|        # Convert vocab_to_str using byte encoding\n",
       "314|        encoded_vocab_to_str = {}\n",
       "315|        for idx, (token_id, token_str) in enumerate(vocab_to_str.items()):\n",
       "316|            # Convert string to bytes then to encoded representation\n",
       "317|            token_bytes = token_str.encode(\"utf-8\")\n",
       "318|            encoded_str = \"\".join(|byte_encoder|b] for b in token_bytes])\n",
       "319|            encoded_vocab_to_str|token_id] = encoded_str\n",
       "320|\n",
       "321|            # Print a few examples as progress (first 5) and then occasional progress\n",
       "322|            if self.verbose:\n",
       "323|                if idx < 5:\n",
       "324|                    self.print_func(\n",
       "325|                        f\"|save] Encoded example {idx + 1}: token_id={token_id} -> {encoded_str!r}\"\n",
       "326|                    )\n",
       "327|                elif (idx + 1) % 1000 == 0:\n",
       "328|                    self.print_func(f\"|save] Encoded {idx + 1} vocab entries...\")\n",
       "329|\n",
       "330|        if self.verbose:\n",
       "331|            self.print_func(\n",
       "332|                f\"|save] Finished encoding {len(encoded_vocab_to_str)} vocab entries.\"\n",
       "333|            )\n",
       "334|\n",
       "335|        # Save vocab.json (encoded_string -> token_id)\n",
       "336|        encoder = {encoded_vocab_to_str|token_id]: token_id for token_id in vocab}\n",
       "337|        vocab_path = f\"{base_path}_vocab.json\"\n",
       "338|        if self.verbose:\n",
       "339|            self.print_func(\n",
       "340|                f\"|save] Writing vocab to {vocab_path} (encoded_string -> token_id)...\"\n",
       "341|            )\n",
       "342|        with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
       "343|            json.dump(encoder, f, ensure_ascii=False, indent=2)\n",
       "344|        if self.verbose:\n",
       "345|            self.print_func(\n",
       "346|                f\"|save] Wrote vocab to {vocab_path} (entries: {len(encoder)}).\"\n",
       "347|            )\n",
       "348|\n",
       "349|        # Save merges.txt using encoded strings\n",
       "350|        merges_path = f\"{base_path}_merges.txt\"\n",
       "351|        if self.verbose:\n",
       "352|            self.print_func(\n",
       "353|                f\"|save] Writing merges to {merges_path} (human-readable encoded pairs)...\"\n",
       "354|            )\n",
       "355|        with open(merges_path, \"w\", encoding=\"utf-8\") as f:\n",
       "356|            f.write(f\"#version: {version}\\n\")\n",
       "357|            for i, (pair, token_id) in enumerate(merged_pairs.items()):\n",
       "358|                token1_encoded = encoded_vocab_to_str|pair|0]]\n",
       "359|                token2_encoded = encoded_vocab_to_str|pair|1]]\n",
       "360|                f.write(f\"{token1_encoded} {token2_encoded}\\n\")\n",
       "361|\n",
       "362|                # small progress prints for merges\n",
       "363|                if self.verbose and i < 5:\n",
       "364|                    self.print_func(\n",
       "365|                        f\"|save] Merge example {i + 1}: {token1_encoded} + {token2_encoded} -> \n",
       "token_id={token_id}\"\n",
       "366|                    )\n",
       "367|\n",
       "368|        if self.verbose:\n",
       "369|            self.print_func(\n",
       "370|                f\"|save] Wrote merges to {merges_path} (entries: {len(merged_pairs)}).\"\n",
       "371|            )\n",
       "372|            self.print_func(\"|save] Completed save().\")\n",
       "373|\n",
       "374|    def load(self, base_path: str):\n",
       "375|        if self.verbose:\n",
       "376|            self.print_func(f\"|load] Starting load() -> base_path: {base_path}\")\n",
       "377|\n",
       "378|        byte_decoder = self._get_byte_decoder()\n",
       "379|\n",
       "380|        # Load vocab.json\n",
       "381|        vocab_path = f\"{base_path}_vocab.json\"\n",
       "382|        if self.verbose:\n",
       "383|            self.print_func(f\"|load] Reading vocab file: {vocab_path}\")\n",
       "384|        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
       "385|            encoder = json.load(f)\n",
       "386|        if self.verbose:\n",
       "387|            self.print_func(f\"|load] Loaded encoder with {len(encoder)} entries.\")\n",
       "388|\n",
       "389|        # Decode back to original strings\n",
       "390|        vocab_to_str = {}\n",
       "391|        for idx, (encoded_str, token_id) in enumerate(encoder.items()):\n",
       "392|            # Decode from encoded representation back to original string\n",
       "393|            decoded_bytes = bytes(|byte_decoder|c] for c in encoded_str])\n",
       "394|            original_str = decoded_bytes.decode(\"utf-8\")\n",
       "395|            vocab_to_str|token_id] = original_str\n",
       "396|\n",
       "397|            if self.verbose:\n",
       "398|                if idx < 5:\n",
       "399|                    self.print_func(\n",
       "400|                        f\"|load] Decoded example {idx+1}: token_id={token_id} -> {original_str!r}\"\n",
       "401|                    )\n",
       "402|                elif (idx + 1) % 1000 == 0:\n",
       "403|                    self.print_func(f\"|load] Decoded {idx+1} vocab entries...\")\n",
       "404|\n",
       "405|        vocab = sorted(vocab_to_str.keys())\n",
       "406|        if self.verbose:\n",
       "407|            self.print_func(f\"|load] Finished decoding vocab (size={len(vocab)}).\")\n",
       "408|\n",
       "409|        # Load merges.txt\n",
       "410|        merges_path = f\"{base_path}_merges.txt\"\n",
       "411|        if self.verbose:\n",
       "412|            self.print_func(f\"|load] Reading merges file: {merges_path}\")\n",
       "413|        merged_pairs = {}\n",
       "414|        next_token = max(vocab) + 1\n",
       "415|\n",
       "416|        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
       "417|            lines = f.readlines()|1:]  # Skip header\n",
       "418|            for i, line in enumerate(lines):\n",
       "419|                if line.strip():\n",
       "420|                    token1_str, token2_str = line.strip().split(\" \", 1)\n",
       "421|                    # Find token IDs for the encoded strings\n",
       "422|                    token1_id = encoder|token1_str]\n",
       "423|                    token2_id = encoder|token2_str]\n",
       "424|                    merged_pairs|(token1_id, token2_id)] = encoder|\n",
       "425|                        token1_str + token2_str\n",
       "426|                    ]\n",
       "427|                    next_token += 1\n",
       "428|\n",
       "429|                    if self.verbose and i < 5:\n",
       "430|                        self.print_func(\n",
       "431|                            f\"|load] Merge example {i+1}: {token1_str}+{token2_str} \"\n",
       "432|                            f\"-> token_id={merged_pairs|(token1_id, token2_id)]}\"\n",
       "433|                        )\n",
       "434|\n",
       "435|        if self.verbose:\n",
       "436|            self.print_func(f\"|load] Loaded {len(merged_pairs)} merge rules.\")\n",
       "437|            self.print_func(\"|load] Completed load().\")\n",
       "438|\n",
       "439|        self.vocab = vocab\n",
       "440|        self.vocab_to_str = vocab_to_str\n",
       "441|        self.merged_pairs = merged_pairs\n",
       "442|        self._token_id_mapping()\n",
       "443|\n",
       "444|        return vocab, vocab_to_str, merged_pairs\n",
       "445|\n",
       "446|    def encode(\n",
       "447|        self,\n",
       "448|        text: str,\n",
       "449|        normalise: bool = False,\n",
       "450|    ) -> List|int]:\n",
       "451|        \"\"\"\n",
       "452|        Encode text to BPE tokens using learned merged pairs.\n",
       "453|        Args:\n",
       "454|            text: Input string\n",
       "455|            merged_pairs: Dict mapping pairs to new tokens\n",
       "456|            splitting_pattern: Compiled regex splitting pattern\n",
       "457|            vocab_to_str: Dict mapping token id to string representation\n",
       "458|        Returns:\n",
       "459|            List of int tokens\n",
       "460|        \"\"\"\n",
       "461|\n",
       "462|        # Create reverse mapping for special tokens (only tokens starting with < and ending with >)\n",
       "463|        special_token_to_id = {\n",
       "464|            token_str: token_id\n",
       "465|            for token_id, token_str in self.vocab_to_str.items()\n",
       "466|            if token_str in self.special_tokens\n",
       "467|        }\n",
       "468|        sorted_pairs = self.merged_pairs.items()\n",
       "469|\n",
       "470|        # handle special tokens by making a regex pattern to escape them\n",
       "471|        special_patterns = |re.escape(token) for token in special_token_to_id.keys()]\n",
       "472|        special_regex = \"(\" + \"|\".join(special_patterns) + \")\"\n",
       "473|        segments = re.split(special_regex, text)\n",
       "474|        self.verbose and self.print_func(f\"{len(segments) = }\")\n",
       "475|        all_tokens = |]\n",
       "476|        for segment in tqdm(segments, position=0):\n",
       "477|            if segment in special_token_to_id:\n",
       "478|                # This segment is a special token - add its ID directly\n",
       "479|                all_tokens.append(special_token_to_id|segment])\n",
       "480|            elif segment:  # Non-empty regular text segment\n",
       "481|                subwords_tokens = self._split_data(segment)\n",
       "482|                for pair, new_token in tqdm(sorted_pairs, position=1, leave=False):\n",
       "483|                    subwords_tokens, _ = self._merge(subwords_tokens, pair, new_token)\n",
       "484|                segment_tokens = |\n",
       "485|                    token for subword in subwords_tokens for token in subword\n",
       "486|                ]\n",
       "487|                all_tokens.extend(segment_tokens)\n",
       "488|        return all_tokens\n",
       "489|\n",
       "490|    def decode(self, tokens: List|int]) -> str:\n",
       "491|        \"\"\"\n",
       "492|        Decode list of BPE tokens back to string\n",
       "493|        Args:\n",
       "494|            tokens: List of token ids\n",
       "495|            vocab_to_str: Dict mapping token id to string\n",
       "496|        Returns:\n",
       "497|            Decoded string\n",
       "498|        \"\"\"\n",
       "499|        # Map each token to its string representation and concatenate\n",
       "500|        return \"\".join(self.vocab_to_str|token] for token in tqdm(tokens))\n",
       "501|\n",
       "502|    def tokenize(\n",
       "503|        self, texts: list|str], padding_direction: str = \"left\", max_length: int = 128\n",
       "504|    ):\n",
       "505|        # since we dont have the vocab number for the special tokens\n",
       "506|        # this is a hacky way to get that\n",
       "507|        # then we can use it in token_to_id\n",
       "508|        special_token_to_id = {\n",
       "509|            token_str: token_id\n",
       "510|            for token_id, token_str in self.vocab_to_str.items()\n",
       "511|            if token_str.startswith(\"<\") and token_str.endswith(\">\")\n",
       "512|        }\n",
       "513|        pad_id = self.token_to_id|special_token_to_id|self.pad_token]]\n",
       "514|        encoded = |self.encode(t)|:max_length] for t in texts]\n",
       "515|        # pre allocate numpy arrays\n",
       "516|        batch_size = len(encoded)\n",
       "517|        input_ids = np.full((batch_size, max_length), pad_id, dtype=np.int32)\n",
       "518|        attention_mask = np.zeros((batch_size, max_length), dtype=np.int32)\n",
       "519|        # fill in the input_ids and attention_mask\n",
       "520|        for i, seq in enumerate(encoded):\n",
       "521|            length = len(seq)\n",
       "522|            if padding_direction == \"right\":\n",
       "523|                input_ids|i, :length] = seq\n",
       "524|                attention_mask|i, :length] = 1\n",
       "525|            elif padding_direction == \"left\":\n",
       "526|                input_ids|i, -length:] = seq\n",
       "527|                attention_mask|i, -length:] = 1\n",
       "528|            else:\n",
       "529|                raise ValueError(\"padding_direction must be 'left' or 'right'\")\n",
       "530|\n",
       "531|        return input_ids, attention_mask\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 6.90 seconds| Input tokens: 2,773 | Output tokens: 22]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 6.90 seconds| Input tokens: 2,773 | Output tokens: 22]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'definition' with arguments: {'symbolName': 'MedievalLMTokenizer'}                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'definition' with arguments: {'symbolName': 'MedievalLMTokenizer'}                                │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: ---\n",
       "\n",
       "Symbol: MedievalLMTokenizer\n",
       "File: /home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py\n",
       "Kind: Class\n",
       "Range: L13:C1 - L531:C41\n",
       "\n",
       " 13|class MedievalLMTokenizer:\n",
       " 14|    \"\"\"\n",
       " 15|    Minimal refactor of the provided BPE training snippet into a class.\n",
       " 16|    Preserves the original algorithm/logic exactly while exposing configuration.\n",
       " 17|\n",
       " 18|    Usage:\n",
       " 19|        trainer = MedievalLMTokenizer(text)\n",
       " 20|        vocab, vocab_to_str, merged_pairs = trainer.train()\n",
       " 21|    \"\"\"\n",
       " 22|\n",
       " 23|    def __init__(\n",
       " 24|        self,\n",
       " 25|        min_merge_threshold: int = 400,\n",
       " 26|        max_token_threshold: int = 512,\n",
       " 27|        splitting_pattern: Optional|re.Pattern] = None,\n",
       " 28|        special_tokens: Optional|List|str]] = None,\n",
       " 29|        text: str = None,\n",
       " 30|        base_path: str = None,\n",
       " 31|        verbose: int = 1,\n",
       " 32|        logger: logging.Logger = None,\n",
       " 33|    ):\n",
       " 34|        \"\"\"\n",
       " 35|        Parameters mirror the original snippet, defaults set to your example.\n",
       " 36|        - text: input text to train BPE on\n",
       " 37|        - min_merge_threshold: minimum pair frequency to continue merging\n",
       " 38|        - max_token_threshold: target vocab size (stop when vocab reaches this)\n",
       " 39|        - splitting_pattern: compiled regex pattern to split text into subwords\n",
       " 40|        - special_tokens: optional list of special tokens to append at the end\n",
       " 41|        - base_path: optional path to load the tokenizer from\n",
       " 42|        - verbose: if True prints the same status lines as the original snippet\n",
       " 43|        \"\"\"\n",
       " 44|        self.min_merge_threshold = min_merge_threshold\n",
       " 45|        self.max_token_threshold = max_token_threshold\n",
       " 46|        self.verbose = verbose\n",
       " 47|        self.base_path = base_path\n",
       " 48|        if verbose:\n",
       " 49|            if logger:\n",
       " 50|                self.print_func = logger.info\n",
       " 51|            else:\n",
       " 52|                self.print_func = print\n",
       " 53|\n",
       " 54|        # Default splitting pattern is exactly as in the snippet\n",
       " 55|        if splitting_pattern is None:\n",
       " 56|            # NOTE: kept exactly the same raw pattern string as provided\n",
       " 57|            self.splitting_pattern = re.compile(\n",
       " 58|                r\"\"\"'|a-zA-Z]+| ?\\p{L}+| ?\\p{N}+| ?|^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)\"\"\"\n",
       " 59|            )\n",
       " 60|        else:\n",
       " 61|            self.splitting_pattern = splitting_pattern\n",
       " 62|\n",
       " 63|        if special_tokens is None:\n",
       " 64|            self.unk_token = \"&lt;UNK&gt;\"  # Unknown token\n",
       " 65|            self.pad_token = \"&lt;PAD&gt;\"  # Padding token\n",
       " 66|            self.bos_token = \"&lt;BOS&gt;\"  # Beginning of sequence\n",
       " 67|            self.eos_token = \"&lt;EOS&gt;\"  # End of sequence\n",
       " 68|            self.modern_token = \"&lt;USER&gt;\"  # Modern English input\n",
       " 69|            self.shakespeare_token = \"&lt;SHAKESPEARE&gt;\"  # Shakespearean output\n",
       " 70|            self.speaker_start_token = \"&lt;SPEAKER&gt;\"  # Character speaking\n",
       " 71|            self.speaker_end_token = \"&lt;/SPEAKER&gt;\"  # Character speaking\n",
       " 72|            self.dialog_start_token = \"&lt;DIALOG&gt;\"  # Stage directions\n",
       " 73|            self.dialog_end_token = \"&lt;/DIALOG&gt;\"  # Stage directions\n",
       " 74|\n",
       " 75|            self.special_tokens = |\n",
       " 76|                self.unk_token,\n",
       " 77|                self.pad_token,\n",
       " 78|                self.bos_token,\n",
       " 79|                self.eos_token,\n",
       " 80|                self.modern_token,\n",
       " 81|                self.shakespeare_token,\n",
       " 82|                self.speaker_start_token,\n",
       " 83|                self.speaker_end_token,\n",
       " 84|                self.dialog_start_token,\n",
       " 85|                self.dialog_end_token,\n",
       " 86|            ]\n",
       " 87|        else:\n",
       " 88|            self.special_tokens = special_tokens\n",
       " 89|\n",
       " 90|        # Internal state that training will populate\n",
       " 91|        self.vocab: List|int] = |]\n",
       " 92|        self.vocab_to_str: Dict|int, str] = {}\n",
       " 93|        self.merged_pairs: Dict|Tuple|int, int], int] = {}\n",
       " 94|\n",
       " 95|        if text is not None:\n",
       " 96|            _ = self.train(text, base_path=self.base_path)\n",
       " 97|        if base_path is not None:\n",
       " 98|            _ = self.load(base_path=self.base_path)\n",
       " 99|\n",
       "100|    def _merge(\n",
       "101|        self,\n",
       "102|        subwords_tokens: List|List|int]],\n",
       "103|        most_frequent_pair: Tuple|int, int],\n",
       "104|        new_token: int,\n",
       "105|        pair_counts: Optional|Dict|Tuple|int, int], int]] = None,\n",
       "106|    ) -&gt; Tuple|List|List|int]], Dict|Tuple|int, int], int]]:\n",
       "107|        \"\"\"\n",
       "108|        Merge occurrences of `most_frequent_pair` in subwords_tokens into `new_token`.\n",
       "109|\n",
       "110|        If pair_counts is not provided, a defaultdict(int) is created and initialized\n",
       "111|        with counts of all adjacent pairs in `subwords_tokens`.\n",
       "112|\n",
       "113|        The function updates pair_counts in-place to reflect the removals/additions\n",
       "114|        of adjacent pairs caused by each merge and returns (merged_subwords_tokens, pair_counts).\n",
       "115|\n",
       "116|        Args:\n",
       "117|            subwords_tokens: list of tokenized subword sequences (lists of ints)\n",
       "118|            most_frequent_pair: the pair (a, b) to merge\n",
       "119|            new_token: integer id to use for the merged token\n",
       "120|            pair_counts: optional dict mapping (token_i, token_j) -&gt; count\n",
       "121|\n",
       "122|        Returns:\n",
       "123|            (merged_subwords_tokens, pair_counts)\n",
       "124|        \"\"\"\n",
       "125|        if pair_counts is None:\n",
       "126|            pair_counts = defaultdict(int)\n",
       "127|        a, b = most_frequent_pair\n",
       "128|        merged_subwords_tokens: List|List|int]] = |]\n",
       "129|        for subsequence in subwords_tokens:\n",
       "130|            # short-circuit for very short sequences\n",
       "131|            if len(subsequence) &lt; 2:\n",
       "132|                # copy to avoid mutating input lists\n",
       "133|                merged_subwords_tokens.append(list(subsequence))\n",
       "134|                continue\n",
       "135|\n",
       "136|            merged_subsequence: List|int] = |]\n",
       "137|            i = 0\n",
       "138|            L = len(subsequence)\n",
       "139|\n",
       "140|            while i &lt; L:\n",
       "141|                # If we can merge at this position\n",
       "142|                if (i &lt;= L - 2) and ((subsequence|i], subsequence|i + 1]) == (a, b)):\n",
       "143|                    # neighbors in the original subsequence (before this merge)\n",
       "144|                    prev_token = merged_subsequence|-1] if i - 1 &gt;= 0 else None\n",
       "145|                    next_token = subsequence|i + 2] if i + 2 &lt; L else None\n",
       "146|\n",
       "147|                    # decrement counts for pairs that will disappear:\n",
       "148|                    # (prev, a), (a, b), (b, next)\n",
       "149|                    if prev_token is not None:\n",
       "150|                        pair_counts|(prev_token, subsequence|i])] -= 1\n",
       "151|                    pair_counts|(subsequence|i], subsequence|i + 1])] -= 1\n",
       "152|                    if next_token is not None:\n",
       "153|                        pair_counts|(subsequence|i + 1], next_token)] -= 1\n",
       "154|\n",
       "155|                    # increment counts for newly created pairs: (prev, new_token), (new_token, next)\n",
       "156|                    if prev_token is not None:\n",
       "157|                        pair_counts|(prev_token, new_token)] += 1\n",
       "158|                    if next_token is not None:\n",
       "159|                        pair_counts|(new_token, next_token)] += 1\n",
       "160|\n",
       "161|                    # append the merged token\n",
       "162|                    merged_subsequence.append(new_token)\n",
       "163|                    i += 2  # skip the merged pair\n",
       "164|                else:\n",
       "165|                    # no merge here; append the current token\n",
       "166|                    # but we should be careful to keep counts consistent:\n",
       "167|                    # when we move past a single token not merged, nothing changes to pair counts\n",
       "168|                    merged_subsequence.append(subsequence|i])\n",
       "169|                    i += 1\n",
       "170|\n",
       "171|            merged_subwords_tokens.append(merged_subsequence)\n",
       "172|\n",
       "173|        return merged_subwords_tokens, pair_counts\n",
       "174|\n",
       "175|    def _token_id_mapping(self):\n",
       "176|        self.id_to_token = self.vocab_to_str\n",
       "177|        self.token_to_id = {v: k for k, v in self.id_to_token.items()}\n",
       "178|\n",
       "179|    def _split_data(self, text):\n",
       "180|        subwords = re.findall(self.splitting_pattern, text)\n",
       "181|        subwords_tokens = |list(word.encode(\"utf-8\")) for word in subwords]\n",
       "182|        if self.verbose &gt;= 2:\n",
       "183|            self.print_func(f\"After splitting:\")\n",
       "184|            self.print_func(f\"{len(subwords_tokens) = }\")\n",
       "185|            for i in range(min(len(subwords), 5)):\n",
       "186|                self.print_func(f\"{subwords|i]} -&gt; {subwords_tokens|i]}\")\n",
       "187|        return subwords_tokens\n",
       "188|\n",
       "189|    def train(self, text: str, base_path: str = \".\"):\n",
       "190|        \"\"\"\n",
       "191|        Run the BPE training loop and return (vocab, vocab_to_str, merged_pairs)\n",
       "192|        Preserves printing and logic of the original snippet.\n",
       "193|        \"\"\"\n",
       "194|        # split the text into subwords\n",
       "195|        subwords_tokens = self._split_data(text)\n",
       "196|\n",
       "197|        # get the tokens and the vocab\n",
       "198|        all_tokens = |token for subword in subwords_tokens for token in subword]\n",
       "199|        self.vocab = list(range(256))\n",
       "200|        self.vocab_to_str = {v: chr(v) for i, v in enumerate(self.vocab)}\n",
       "201|\n",
       "202|        merged_pairs = {}\n",
       "203|\n",
       "204|        if self.verbose:\n",
       "205|            all_tokens = |token for subword in subwords_tokens for token in subword]\n",
       "206|            self.print_func(f\"Initial vocab stats:\")\n",
       "207|            self.print_func(f\"  Unique bytes in text: {len(self.vocab)}\")\n",
       "208|            self.print_func(f\"  Total tokens: {len(all_tokens)}\")\n",
       "209|            self.print_func(f\"  Subwords: {len(subwords_tokens)}\")\n",
       "210|            self.print_func(f\"  Target vocab size: {self.max_token_threshold}\")\n",
       "211|            self.print_func(f\"  Min merge threshold: {self.min_merge_threshold}\")\n",
       "212|            self.print_func(\"Starting BPE merging...\")\n",
       "213|            self.print_func(\"Pair\\t\\t\\tFreq\\tNew Token\\tToken String\\t\\tVocab Size\")\n",
       "214|            self.print_func(\"-\" * 80)\n",
       "215|\n",
       "216|        next_token = max(self.vocab) + 1 if self.vocab else 0\n",
       "217|        self.verbose and self.print_func(f\"{next_token = }\")\n",
       "218|        pair_counts = Counter()\n",
       "219|        for tokens in subwords_tokens:\n",
       "220|            for i in range(len(tokens) - 1):\n",
       "221|                pair = (tokens|i], tokens|i + 1])\n",
       "222|                pair_counts|pair] += 1\n",
       "223|        while len(self.vocab) &lt; self.max_token_threshold:\n",
       "224|            if not pair_counts:\n",
       "225|                break\n",
       "226|            most_frequent_pair, actual_count = pair_counts.most_common(1)|0]\n",
       "227|            if actual_count &lt;= self.min_merge_threshold:\n",
       "228|                break\n",
       "229|            subwords_tokens, pair_counts = self._merge(\n",
       "230|                subwords_tokens, most_frequent_pair, next_token, pair_counts\n",
       "231|            )\n",
       "232|            new_token_string = (\n",
       "233|                self.vocab_to_str|most_frequent_pair|0]]\n",
       "234|                + self.vocab_to_str|most_frequent_pair|1]]\n",
       "235|            )\n",
       "236|            self.vocab.append(next_token)\n",
       "237|            self.vocab_to_str|next_token] = new_token_string\n",
       "238|            merged_pairs|most_frequent_pair] = next_token\n",
       "239|            if self.verbose:\n",
       "240|                self.print_func(\n",
       "241|                    \n",
       "f\"{most_frequent_pair}\\t\\t{actual_count}\\t{next_token}\\t\\t'{new_token_string}'\\t\\t\\t{len(self.vocab)}\"\n",
       "242|                )\n",
       "243|            next_token += 1\n",
       "244|        special_token_to_str = {\n",
       "245|            i + max(self.vocab) + 1: v for i, v in enumerate(self.special_tokens)\n",
       "246|        }\n",
       "247|        self.vocab_to_str.update(special_token_to_str)\n",
       "248|        self.vocab.extend(list(special_token_to_str.keys()))\n",
       "249|\n",
       "250|        if self.verbose:\n",
       "251|            self.print_func(\"-\" * 80)\n",
       "252|            self.print_func(\n",
       "253|                f\"BPE training completed. Final vocab size: {len(self.vocab)}\"\n",
       "254|            )\n",
       "255|\n",
       "256|        # Persist state back to object\n",
       "257|        self.merged_pairs = merged_pairs\n",
       "258|\n",
       "259|        if self.base_path is not None:\n",
       "260|            self.save(self.base_path, self.vocab, self.vocab_to_str, self.merged_pairs)\n",
       "261|\n",
       "262|        return self.vocab, self.vocab_to_str, merged_pairs\n",
       "263|\n",
       "264|    def _get_byte_encoder(self):\n",
       "265|        \"\"\"GPT-2 style byte-to-unicode mapping for safely representing bytes as strings\"\"\"\n",
       "266|        bs = (\n",
       "267|            list(range(ord(\"!\"), ord(\"~\") + 1))\n",
       "268|            + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
       "269|            + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
       "270|        )\n",
       "271|        cs = bs|:]\n",
       "272|        n = 0\n",
       "273|        for b in range(2**8):\n",
       "274|            if b not in bs:\n",
       "275|                bs.append(b)\n",
       "276|                cs.append(2**8 + n)\n",
       "277|                n += 1\n",
       "278|        cs = |chr(n) for n in cs]\n",
       "279|        return dict(zip(bs, cs))\n",
       "280|\n",
       "281|    def _get_byte_decoder(self):\n",
       "282|        byte_encoder = self._get_byte_encoder()\n",
       "283|        return {v: k for k, v in byte_encoder.items()}\n",
       "284|\n",
       "285|    def save(\n",
       "286|        self,\n",
       "287|        base_path: str,\n",
       "288|        vocab: list | set = None,\n",
       "289|        vocab_to_str: dict = None,\n",
       "290|        merged_pairs: dict | list = None,\n",
       "291|        version: str = None,\n",
       "292|    ):\n",
       "293|        if vocab is None:\n",
       "294|            vocab = self.vocab\n",
       "295|        if vocab_to_str is None:\n",
       "296|            vocab_to_str = self.vocab_to_str\n",
       "297|        if merged_pairs is None:\n",
       "298|            merged_pairs = self.merged_pairs\n",
       "299|        if version is None:\n",
       "300|            version = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
       "301|\n",
       "302|        if self.verbose:\n",
       "303|            self.print_func(f\"|save] Starting save() -&gt; base_path: {base_path}\")\n",
       "304|            self.print_func(\n",
       "305|                f\"|save] Vocab entries: {len(vocab)}; Merged pairs: {len(merged_pairs)}\"\n",
       "306|            )\n",
       "307|\n",
       "308|        byte_encoder = self._get_byte_encoder()\n",
       "309|\n",
       "310|        if self.verbose:\n",
       "311|            self.print_func(\"|save] Converting vocab_to_str using byte encoder...\")\n",
       "312|\n",
       "313|        # Convert vocab_to_str using byte encoding\n",
       "314|        encoded_vocab_to_str = {}\n",
       "315|        for idx, (token_id, token_str) in enumerate(vocab_to_str.items()):\n",
       "316|            # Convert string to bytes then to encoded representation\n",
       "317|            token_bytes = token_str.encode(\"utf-8\")\n",
       "318|            encoded_str = \"\".join(|byte_encoder|b] for b in token_bytes])\n",
       "319|            encoded_vocab_to_str|token_id] = encoded_str\n",
       "320|\n",
       "321|            # Print a few examples as progress (first 5) and then occasional progress\n",
       "322|            if self.verbose:\n",
       "323|                if idx &lt; 5:\n",
       "324|                    self.print_func(\n",
       "325|                        f\"|save] Encoded example {idx + 1}: token_id={token_id} -&gt; {encoded_str!r}\"\n",
       "326|                    )\n",
       "327|                elif (idx + 1) % 1000 == 0:\n",
       "328|                    self.print_func(f\"|save] Encoded {idx + 1} vocab entries...\")\n",
       "329|\n",
       "330|        if self.verbose:\n",
       "331|            self.print_func(\n",
       "332|                f\"|save] Finished encoding {len(encoded_vocab_to_str)} vocab entries.\"\n",
       "333|            )\n",
       "334|\n",
       "335|        # Save vocab.json (encoded_string -&gt; token_id)\n",
       "336|        encoder = {encoded_vocab_to_str|token_id]: token_id for token_id in vocab}\n",
       "337|        vocab_path = f\"{base_path}_vocab.json\"\n",
       "338|        if self.verbose:\n",
       "339|            self.print_func(\n",
       "340|                f\"|save] Writing vocab to {vocab_path} (encoded_string -&gt; token_id)...\"\n",
       "341|            )\n",
       "342|        with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
       "343|            json.dump(encoder, f, ensure_ascii=False, indent=2)\n",
       "344|        if self.verbose:\n",
       "345|            self.print_func(\n",
       "346|                f\"|save] Wrote vocab to {vocab_path} (entries: {len(encoder)}).\"\n",
       "347|            )\n",
       "348|\n",
       "349|        # Save merges.txt using encoded strings\n",
       "350|        merges_path = f\"{base_path}_merges.txt\"\n",
       "351|        if self.verbose:\n",
       "352|            self.print_func(\n",
       "353|                f\"|save] Writing merges to {merges_path} (human-readable encoded pairs)...\"\n",
       "354|            )\n",
       "355|        with open(merges_path, \"w\", encoding=\"utf-8\") as f:\n",
       "356|            f.write(f\"#version: {version}\\n\")\n",
       "357|            for i, (pair, token_id) in enumerate(merged_pairs.items()):\n",
       "358|                token1_encoded = encoded_vocab_to_str|pair|0]]\n",
       "359|                token2_encoded = encoded_vocab_to_str|pair|1]]\n",
       "360|                f.write(f\"{token1_encoded} {token2_encoded}\\n\")\n",
       "361|\n",
       "362|                # small progress prints for merges\n",
       "363|                if self.verbose and i &lt; 5:\n",
       "364|                    self.print_func(\n",
       "365|                        f\"|save] Merge example {i + 1}: {token1_encoded} + {token2_encoded} -&gt; \n",
       "token_id={token_id}\"\n",
       "366|                    )\n",
       "367|\n",
       "368|        if self.verbose:\n",
       "369|            self.print_func(\n",
       "370|                f\"|save] Wrote merges to {merges_path} (entries: {len(merged_pairs)}).\"\n",
       "371|            )\n",
       "372|            self.print_func(\"|save] Completed save().\")\n",
       "373|\n",
       "374|    def load(self, base_path: str):\n",
       "375|        if self.verbose:\n",
       "376|            self.print_func(f\"|load] Starting load() -&gt; base_path: {base_path}\")\n",
       "377|\n",
       "378|        byte_decoder = self._get_byte_decoder()\n",
       "379|\n",
       "380|        # Load vocab.json\n",
       "381|        vocab_path = f\"{base_path}_vocab.json\"\n",
       "382|        if self.verbose:\n",
       "383|            self.print_func(f\"|load] Reading vocab file: {vocab_path}\")\n",
       "384|        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
       "385|            encoder = json.load(f)\n",
       "386|        if self.verbose:\n",
       "387|            self.print_func(f\"|load] Loaded encoder with {len(encoder)} entries.\")\n",
       "388|\n",
       "389|        # Decode back to original strings\n",
       "390|        vocab_to_str = {}\n",
       "391|        for idx, (encoded_str, token_id) in enumerate(encoder.items()):\n",
       "392|            # Decode from encoded representation back to original string\n",
       "393|            decoded_bytes = bytes(|byte_decoder|c] for c in encoded_str])\n",
       "394|            original_str = decoded_bytes.decode(\"utf-8\")\n",
       "395|            vocab_to_str|token_id] = original_str\n",
       "396|\n",
       "397|            if self.verbose:\n",
       "398|                if idx &lt; 5:\n",
       "399|                    self.print_func(\n",
       "400|                        f\"|load] Decoded example {idx+1}: token_id={token_id} -&gt; {original_str!r}\"\n",
       "401|                    )\n",
       "402|                elif (idx + 1) % 1000 == 0:\n",
       "403|                    self.print_func(f\"|load] Decoded {idx+1} vocab entries...\")\n",
       "404|\n",
       "405|        vocab = sorted(vocab_to_str.keys())\n",
       "406|        if self.verbose:\n",
       "407|            self.print_func(f\"|load] Finished decoding vocab (size={len(vocab)}).\")\n",
       "408|\n",
       "409|        # Load merges.txt\n",
       "410|        merges_path = f\"{base_path}_merges.txt\"\n",
       "411|        if self.verbose:\n",
       "412|            self.print_func(f\"|load] Reading merges file: {merges_path}\")\n",
       "413|        merged_pairs = {}\n",
       "414|        next_token = max(vocab) + 1\n",
       "415|\n",
       "416|        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
       "417|            lines = f.readlines()|1:]  # Skip header\n",
       "418|            for i, line in enumerate(lines):\n",
       "419|                if line.strip():\n",
       "420|                    token1_str, token2_str = line.strip().split(\" \", 1)\n",
       "421|                    # Find token IDs for the encoded strings\n",
       "422|                    token1_id = encoder|token1_str]\n",
       "423|                    token2_id = encoder|token2_str]\n",
       "424|                    merged_pairs|(token1_id, token2_id)] = encoder|\n",
       "425|                        token1_str + token2_str\n",
       "426|                    ]\n",
       "427|                    next_token += 1\n",
       "428|\n",
       "429|                    if self.verbose and i &lt; 5:\n",
       "430|                        self.print_func(\n",
       "431|                            f\"|load] Merge example {i+1}: {token1_str}+{token2_str} \"\n",
       "432|                            f\"-&gt; token_id={merged_pairs|(token1_id, token2_id)]}\"\n",
       "433|                        )\n",
       "434|\n",
       "435|        if self.verbose:\n",
       "436|            self.print_func(f\"|load] Loaded {len(merged_pairs)} merge rules.\")\n",
       "437|            self.print_func(\"|load] Completed load().\")\n",
       "438|\n",
       "439|        self.vocab = vocab\n",
       "440|        self.vocab_to_str = vocab_to_str\n",
       "441|        self.merged_pairs = merged_pairs\n",
       "442|        self._token_id_mapping()\n",
       "443|\n",
       "444|        return vocab, vocab_to_str, merged_pairs\n",
       "445|\n",
       "446|    def encode(\n",
       "447|        self,\n",
       "448|        text: str,\n",
       "449|        normalise: bool = False,\n",
       "450|    ) -&gt; List|int]:\n",
       "451|        \"\"\"\n",
       "452|        Encode text to BPE tokens using learned merged pairs.\n",
       "453|        Args:\n",
       "454|            text: Input string\n",
       "455|            merged_pairs: Dict mapping pairs to new tokens\n",
       "456|            splitting_pattern: Compiled regex splitting pattern\n",
       "457|            vocab_to_str: Dict mapping token id to string representation\n",
       "458|        Returns:\n",
       "459|            List of int tokens\n",
       "460|        \"\"\"\n",
       "461|\n",
       "462|        # Create reverse mapping for special tokens (only tokens starting with &lt; and ending with &gt;)\n",
       "463|        special_token_to_id = {\n",
       "464|            token_str: token_id\n",
       "465|            for token_id, token_str in self.vocab_to_str.items()\n",
       "466|            if token_str in self.special_tokens\n",
       "467|        }\n",
       "468|        sorted_pairs = self.merged_pairs.items()\n",
       "469|\n",
       "470|        # handle special tokens by making a regex pattern to escape them\n",
       "471|        special_patterns = |re.escape(token) for token in special_token_to_id.keys()]\n",
       "472|        special_regex = \"(\" + \"|\".join(special_patterns) + \")\"\n",
       "473|        segments = re.split(special_regex, text)\n",
       "474|        self.verbose and self.print_func(f\"{len(segments) = }\")\n",
       "475|        all_tokens = |]\n",
       "476|        for segment in tqdm(segments, position=0):\n",
       "477|            if segment in special_token_to_id:\n",
       "478|                # This segment is a special token - add its ID directly\n",
       "479|                all_tokens.append(special_token_to_id|segment])\n",
       "480|            elif segment:  # Non-empty regular text segment\n",
       "481|                subwords_tokens = self._split_data(segment)\n",
       "482|                for pair, new_token in tqdm(sorted_pairs, position=1, leave=False):\n",
       "483|                    subwords_tokens, _ = self._merge(subwords_tokens, pair, new_token)\n",
       "484|                segment_tokens = |\n",
       "485|                    token for subword in subwords_tokens for token in subword\n",
       "486|                ]\n",
       "487|                all_tokens.extend(segment_tokens)\n",
       "488|        return all_tokens\n",
       "489|\n",
       "490|    def decode(self, tokens: List|int]) -&gt; str:\n",
       "491|        \"\"\"\n",
       "492|        Decode list of BPE tokens back to string\n",
       "493|        Args:\n",
       "494|            tokens: List of token ids\n",
       "495|            vocab_to_str: Dict mapping token id to string\n",
       "496|        Returns:\n",
       "497|            Decoded string\n",
       "498|        \"\"\"\n",
       "499|        # Map each token to its string representation and concatenate\n",
       "500|        return \"\".join(self.vocab_to_str|token] for token in tqdm(tokens))\n",
       "501|\n",
       "502|    def tokenize(\n",
       "503|        self, texts: list|str], padding_direction: str = \"left\", max_length: int = 128\n",
       "504|    ):\n",
       "505|        # since we dont have the vocab number for the special tokens\n",
       "506|        # this is a hacky way to get that\n",
       "507|        # then we can use it in token_to_id\n",
       "508|        special_token_to_id = {\n",
       "509|            token_str: token_id\n",
       "510|            for token_id, token_str in self.vocab_to_str.items()\n",
       "511|            if token_str.startswith(\"&lt;\") and token_str.endswith(\"&gt;\")\n",
       "512|        }\n",
       "513|        pad_id = self.token_to_id|special_token_to_id|self.pad_token]]\n",
       "514|        encoded = |self.encode(t)|:max_length] for t in texts]\n",
       "515|        # pre allocate numpy arrays\n",
       "516|        batch_size = len(encoded)\n",
       "517|        input_ids = np.full((batch_size, max_length), pad_id, dtype=np.int32)\n",
       "518|        attention_mask = np.zeros((batch_size, max_length), dtype=np.int32)\n",
       "519|        # fill in the input_ids and attention_mask\n",
       "520|        for i, seq in enumerate(encoded):\n",
       "521|            length = len(seq)\n",
       "522|            if padding_direction == \"right\":\n",
       "523|                input_ids|i, :length] = seq\n",
       "524|                attention_mask|i, :length] = 1\n",
       "525|            elif padding_direction == \"left\":\n",
       "526|                input_ids|i, -length:] = seq\n",
       "527|                attention_mask|i, -length:] = 1\n",
       "528|            else:\n",
       "529|                raise ValueError(\"padding_direction must be 'left' or 'right'\")\n",
       "530|\n",
       "531|        return input_ids, attention_mask\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: ---\n",
       "\n",
       "Symbol: MedievalLMTokenizer\n",
       "File: /home/rudy/code/karpathy-gpt/src/medievallm/tokenizer.py\n",
       "Kind: Class\n",
       "Range: L13:C1 - L531:C41\n",
       "\n",
       " 13|class MedievalLMTokenizer:\n",
       " 14|    \"\"\"\n",
       " 15|    Minimal refactor of the provided BPE training snippet into a class.\n",
       " 16|    Preserves the original algorithm/logic exactly while exposing configuration.\n",
       " 17|\n",
       " 18|    Usage:\n",
       " 19|        trainer = MedievalLMTokenizer(text)\n",
       " 20|        vocab, vocab_to_str, merged_pairs = trainer.train()\n",
       " 21|    \"\"\"\n",
       " 22|\n",
       " 23|    def __init__(\n",
       " 24|        self,\n",
       " 25|        min_merge_threshold: int = 400,\n",
       " 26|        max_token_threshold: int = 512,\n",
       " 27|        splitting_pattern: Optional|re.Pattern] = None,\n",
       " 28|        special_tokens: Optional|List|str]] = None,\n",
       " 29|        text: str = None,\n",
       " 30|        base_path: str = None,\n",
       " 31|        verbose: int = 1,\n",
       " 32|        logger: logging.Logger = None,\n",
       " 33|    ):\n",
       " 34|        \"\"\"\n",
       " 35|        Parameters mirror the original snippet, defaults set to your example.\n",
       " 36|        - text: input text to train BPE on\n",
       " 37|        - min_merge_threshold: minimum pair frequency to continue merging\n",
       " 38|        - max_token_threshold: target vocab size (stop when vocab reaches this)\n",
       " 39|        - splitting_pattern: compiled regex pattern to split text into subwords\n",
       " 40|        - special_tokens: optional list of special tokens to append at the end\n",
       " 41|        - base_path: optional path to load the tokenizer from\n",
       " 42|        - verbose: if True prints the same status lines as the original snippet\n",
       " 43|        \"\"\"\n",
       " 44|        self.min_merge_threshold = min_merge_threshold\n",
       " 45|        self.max_token_threshold = max_token_threshold\n",
       " 46|        self.verbose = verbose\n",
       " 47|        self.base_path = base_path\n",
       " 48|        if verbose:\n",
       " 49|            if logger:\n",
       " 50|                self.print_func = logger.info\n",
       " 51|            else:\n",
       " 52|                self.print_func = print\n",
       " 53|\n",
       " 54|        # Default splitting pattern is exactly as in the snippet\n",
       " 55|        if splitting_pattern is None:\n",
       " 56|            # NOTE: kept exactly the same raw pattern string as provided\n",
       " 57|            self.splitting_pattern = re.compile(\n",
       " 58|                r\"\"\"'|a-zA-Z]+| ?\\p{L}+| ?\\p{N}+| ?|^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)\"\"\"\n",
       " 59|            )\n",
       " 60|        else:\n",
       " 61|            self.splitting_pattern = splitting_pattern\n",
       " 62|\n",
       " 63|        if special_tokens is None:\n",
       " 64|            self.unk_token = \"<UNK>\"  # Unknown token\n",
       " 65|            self.pad_token = \"<PAD>\"  # Padding token\n",
       " 66|            self.bos_token = \"<BOS>\"  # Beginning of sequence\n",
       " 67|            self.eos_token = \"<EOS>\"  # End of sequence\n",
       " 68|            self.modern_token = \"<USER>\"  # Modern English input\n",
       " 69|            self.shakespeare_token = \"<SHAKESPEARE>\"  # Shakespearean output\n",
       " 70|            self.speaker_start_token = \"<SPEAKER>\"  # Character speaking\n",
       " 71|            self.speaker_end_token = \"</SPEAKER>\"  # Character speaking\n",
       " 72|            self.dialog_start_token = \"<DIALOG>\"  # Stage directions\n",
       " 73|            self.dialog_end_token = \"</DIALOG>\"  # Stage directions\n",
       " 74|\n",
       " 75|            self.special_tokens = |\n",
       " 76|                self.unk_token,\n",
       " 77|                self.pad_token,\n",
       " 78|                self.bos_token,\n",
       " 79|                self.eos_token,\n",
       " 80|                self.modern_token,\n",
       " 81|                self.shakespeare_token,\n",
       " 82|                self.speaker_start_token,\n",
       " 83|                self.speaker_end_token,\n",
       " 84|                self.dialog_start_token,\n",
       " 85|                self.dialog_end_token,\n",
       " 86|            ]\n",
       " 87|        else:\n",
       " 88|            self.special_tokens = special_tokens\n",
       " 89|\n",
       " 90|        # Internal state that training will populate\n",
       " 91|        self.vocab: List|int] = |]\n",
       " 92|        self.vocab_to_str: Dict|int, str] = {}\n",
       " 93|        self.merged_pairs: Dict|Tuple|int, int], int] = {}\n",
       " 94|\n",
       " 95|        if text is not None:\n",
       " 96|            _ = self.train(text, base_path=self.base_path)\n",
       " 97|        if base_path is not None:\n",
       " 98|            _ = self.load(base_path=self.base_path)\n",
       " 99|\n",
       "100|    def _merge(\n",
       "101|        self,\n",
       "102|        subwords_tokens: List|List|int]],\n",
       "103|        most_frequent_pair: Tuple|int, int],\n",
       "104|        new_token: int,\n",
       "105|        pair_counts: Optional|Dict|Tuple|int, int], int]] = None,\n",
       "106|    ) -> Tuple|List|List|int]], Dict|Tuple|int, int], int]]:\n",
       "107|        \"\"\"\n",
       "108|        Merge occurrences of `most_frequent_pair` in subwords_tokens into `new_token`.\n",
       "109|\n",
       "110|        If pair_counts is not provided, a defaultdict(int) is created and initialized\n",
       "111|        with counts of all adjacent pairs in `subwords_tokens`.\n",
       "112|\n",
       "113|        The function updates pair_counts in-place to reflect the removals/additions\n",
       "114|        of adjacent pairs caused by each merge and returns (merged_subwords_tokens, pair_counts).\n",
       "115|\n",
       "116|        Args:\n",
       "117|            subwords_tokens: list of tokenized subword sequences (lists of ints)\n",
       "118|            most_frequent_pair: the pair (a, b) to merge\n",
       "119|            new_token: integer id to use for the merged token\n",
       "120|            pair_counts: optional dict mapping (token_i, token_j) -> count\n",
       "121|\n",
       "122|        Returns:\n",
       "123|            (merged_subwords_tokens, pair_counts)\n",
       "124|        \"\"\"\n",
       "125|        if pair_counts is None:\n",
       "126|            pair_counts = defaultdict(int)\n",
       "127|        a, b = most_frequent_pair\n",
       "128|        merged_subwords_tokens: List|List|int]] = |]\n",
       "129|        for subsequence in subwords_tokens:\n",
       "130|            # short-circuit for very short sequences\n",
       "131|            if len(subsequence) < 2:\n",
       "132|                # copy to avoid mutating input lists\n",
       "133|                merged_subwords_tokens.append(list(subsequence))\n",
       "134|                continue\n",
       "135|\n",
       "136|            merged_subsequence: List|int] = |]\n",
       "137|            i = 0\n",
       "138|            L = len(subsequence)\n",
       "139|\n",
       "140|            while i < L:\n",
       "141|                # If we can merge at this position\n",
       "142|                if (i <= L - 2) and ((subsequence|i], subsequence|i + 1]) == (a, b)):\n",
       "143|                    # neighbors in the original subsequence (before this merge)\n",
       "144|                    prev_token = merged_subsequence|-1] if i - 1 >= 0 else None\n",
       "145|                    next_token = subsequence|i + 2] if i + 2 < L else None\n",
       "146|\n",
       "147|                    # decrement counts for pairs that will disappear:\n",
       "148|                    # (prev, a), (a, b), (b, next)\n",
       "149|                    if prev_token is not None:\n",
       "150|                        pair_counts|(prev_token, subsequence|i])] -= 1\n",
       "151|                    pair_counts|(subsequence|i], subsequence|i + 1])] -= 1\n",
       "152|                    if next_token is not None:\n",
       "153|                        pair_counts|(subsequence|i + 1], next_token)] -= 1\n",
       "154|\n",
       "155|                    # increment counts for newly created pairs: (prev, new_token), (new_token, next)\n",
       "156|                    if prev_token is not None:\n",
       "157|                        pair_counts|(prev_token, new_token)] += 1\n",
       "158|                    if next_token is not None:\n",
       "159|                        pair_counts|(new_token, next_token)] += 1\n",
       "160|\n",
       "161|                    # append the merged token\n",
       "162|                    merged_subsequence.append(new_token)\n",
       "163|                    i += 2  # skip the merged pair\n",
       "164|                else:\n",
       "165|                    # no merge here; append the current token\n",
       "166|                    # but we should be careful to keep counts consistent:\n",
       "167|                    # when we move past a single token not merged, nothing changes to pair counts\n",
       "168|                    merged_subsequence.append(subsequence|i])\n",
       "169|                    i += 1\n",
       "170|\n",
       "171|            merged_subwords_tokens.append(merged_subsequence)\n",
       "172|\n",
       "173|        return merged_subwords_tokens, pair_counts\n",
       "174|\n",
       "175|    def _token_id_mapping(self):\n",
       "176|        self.id_to_token = self.vocab_to_str\n",
       "177|        self.token_to_id = {v: k for k, v in self.id_to_token.items()}\n",
       "178|\n",
       "179|    def _split_data(self, text):\n",
       "180|        subwords = re.findall(self.splitting_pattern, text)\n",
       "181|        subwords_tokens = |list(word.encode(\"utf-8\")) for word in subwords]\n",
       "182|        if self.verbose >= 2:\n",
       "183|            self.print_func(f\"After splitting:\")\n",
       "184|            self.print_func(f\"{len(subwords_tokens) = }\")\n",
       "185|            for i in range(min(len(subwords), 5)):\n",
       "186|                self.print_func(f\"{subwords|i]} -> {subwords_tokens|i]}\")\n",
       "187|        return subwords_tokens\n",
       "188|\n",
       "189|    def train(self, text: str, base_path: str = \".\"):\n",
       "190|        \"\"\"\n",
       "191|        Run the BPE training loop and return (vocab, vocab_to_str, merged_pairs)\n",
       "192|        Preserves printing and logic of the original snippet.\n",
       "193|        \"\"\"\n",
       "194|        # split the text into subwords\n",
       "195|        subwords_tokens = self._split_data(text)\n",
       "196|\n",
       "197|        # get the tokens and the vocab\n",
       "198|        all_tokens = |token for subword in subwords_tokens for token in subword]\n",
       "199|        self.vocab = list(range(256))\n",
       "200|        self.vocab_to_str = {v: chr(v) for i, v in enumerate(self.vocab)}\n",
       "201|\n",
       "202|        merged_pairs = {}\n",
       "203|\n",
       "204|        if self.verbose:\n",
       "205|            all_tokens = |token for subword in subwords_tokens for token in subword]\n",
       "206|            self.print_func(f\"Initial vocab stats:\")\n",
       "207|            self.print_func(f\"  Unique bytes in text: {len(self.vocab)}\")\n",
       "208|            self.print_func(f\"  Total tokens: {len(all_tokens)}\")\n",
       "209|            self.print_func(f\"  Subwords: {len(subwords_tokens)}\")\n",
       "210|            self.print_func(f\"  Target vocab size: {self.max_token_threshold}\")\n",
       "211|            self.print_func(f\"  Min merge threshold: {self.min_merge_threshold}\")\n",
       "212|            self.print_func(\"Starting BPE merging...\")\n",
       "213|            self.print_func(\"Pair\\t\\t\\tFreq\\tNew Token\\tToken String\\t\\tVocab Size\")\n",
       "214|            self.print_func(\"-\" * 80)\n",
       "215|\n",
       "216|        next_token = max(self.vocab) + 1 if self.vocab else 0\n",
       "217|        self.verbose and self.print_func(f\"{next_token = }\")\n",
       "218|        pair_counts = Counter()\n",
       "219|        for tokens in subwords_tokens:\n",
       "220|            for i in range(len(tokens) - 1):\n",
       "221|                pair = (tokens|i], tokens|i + 1])\n",
       "222|                pair_counts|pair] += 1\n",
       "223|        while len(self.vocab) < self.max_token_threshold:\n",
       "224|            if not pair_counts:\n",
       "225|                break\n",
       "226|            most_frequent_pair, actual_count = pair_counts.most_common(1)|0]\n",
       "227|            if actual_count <= self.min_merge_threshold:\n",
       "228|                break\n",
       "229|            subwords_tokens, pair_counts = self._merge(\n",
       "230|                subwords_tokens, most_frequent_pair, next_token, pair_counts\n",
       "231|            )\n",
       "232|            new_token_string = (\n",
       "233|                self.vocab_to_str|most_frequent_pair|0]]\n",
       "234|                + self.vocab_to_str|most_frequent_pair|1]]\n",
       "235|            )\n",
       "236|            self.vocab.append(next_token)\n",
       "237|            self.vocab_to_str|next_token] = new_token_string\n",
       "238|            merged_pairs|most_frequent_pair] = next_token\n",
       "239|            if self.verbose:\n",
       "240|                self.print_func(\n",
       "241|                    \n",
       "f\"{most_frequent_pair}\\t\\t{actual_count}\\t{next_token}\\t\\t'{new_token_string}'\\t\\t\\t{len(self.vocab)}\"\n",
       "242|                )\n",
       "243|            next_token += 1\n",
       "244|        special_token_to_str = {\n",
       "245|            i + max(self.vocab) + 1: v for i, v in enumerate(self.special_tokens)\n",
       "246|        }\n",
       "247|        self.vocab_to_str.update(special_token_to_str)\n",
       "248|        self.vocab.extend(list(special_token_to_str.keys()))\n",
       "249|\n",
       "250|        if self.verbose:\n",
       "251|            self.print_func(\"-\" * 80)\n",
       "252|            self.print_func(\n",
       "253|                f\"BPE training completed. Final vocab size: {len(self.vocab)}\"\n",
       "254|            )\n",
       "255|\n",
       "256|        # Persist state back to object\n",
       "257|        self.merged_pairs = merged_pairs\n",
       "258|\n",
       "259|        if self.base_path is not None:\n",
       "260|            self.save(self.base_path, self.vocab, self.vocab_to_str, self.merged_pairs)\n",
       "261|\n",
       "262|        return self.vocab, self.vocab_to_str, merged_pairs\n",
       "263|\n",
       "264|    def _get_byte_encoder(self):\n",
       "265|        \"\"\"GPT-2 style byte-to-unicode mapping for safely representing bytes as strings\"\"\"\n",
       "266|        bs = (\n",
       "267|            list(range(ord(\"!\"), ord(\"~\") + 1))\n",
       "268|            + list(range(ord(\"¡\"), ord(\"¬\") + 1))\n",
       "269|            + list(range(ord(\"®\"), ord(\"ÿ\") + 1))\n",
       "270|        )\n",
       "271|        cs = bs|:]\n",
       "272|        n = 0\n",
       "273|        for b in range(2**8):\n",
       "274|            if b not in bs:\n",
       "275|                bs.append(b)\n",
       "276|                cs.append(2**8 + n)\n",
       "277|                n += 1\n",
       "278|        cs = |chr(n) for n in cs]\n",
       "279|        return dict(zip(bs, cs))\n",
       "280|\n",
       "281|    def _get_byte_decoder(self):\n",
       "282|        byte_encoder = self._get_byte_encoder()\n",
       "283|        return {v: k for k, v in byte_encoder.items()}\n",
       "284|\n",
       "285|    def save(\n",
       "286|        self,\n",
       "287|        base_path: str,\n",
       "288|        vocab: list | set = None,\n",
       "289|        vocab_to_str: dict = None,\n",
       "290|        merged_pairs: dict | list = None,\n",
       "291|        version: str = None,\n",
       "292|    ):\n",
       "293|        if vocab is None:\n",
       "294|            vocab = self.vocab\n",
       "295|        if vocab_to_str is None:\n",
       "296|            vocab_to_str = self.vocab_to_str\n",
       "297|        if merged_pairs is None:\n",
       "298|            merged_pairs = self.merged_pairs\n",
       "299|        if version is None:\n",
       "300|            version = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
       "301|\n",
       "302|        if self.verbose:\n",
       "303|            self.print_func(f\"|save] Starting save() -> base_path: {base_path}\")\n",
       "304|            self.print_func(\n",
       "305|                f\"|save] Vocab entries: {len(vocab)}; Merged pairs: {len(merged_pairs)}\"\n",
       "306|            )\n",
       "307|\n",
       "308|        byte_encoder = self._get_byte_encoder()\n",
       "309|\n",
       "310|        if self.verbose:\n",
       "311|            self.print_func(\"|save] Converting vocab_to_str using byte encoder...\")\n",
       "312|\n",
       "313|        # Convert vocab_to_str using byte encoding\n",
       "314|        encoded_vocab_to_str = {}\n",
       "315|        for idx, (token_id, token_str) in enumerate(vocab_to_str.items()):\n",
       "316|            # Convert string to bytes then to encoded representation\n",
       "317|            token_bytes = token_str.encode(\"utf-8\")\n",
       "318|            encoded_str = \"\".join(|byte_encoder|b] for b in token_bytes])\n",
       "319|            encoded_vocab_to_str|token_id] = encoded_str\n",
       "320|\n",
       "321|            # Print a few examples as progress (first 5) and then occasional progress\n",
       "322|            if self.verbose:\n",
       "323|                if idx < 5:\n",
       "324|                    self.print_func(\n",
       "325|                        f\"|save] Encoded example {idx + 1}: token_id={token_id} -> {encoded_str!r}\"\n",
       "326|                    )\n",
       "327|                elif (idx + 1) % 1000 == 0:\n",
       "328|                    self.print_func(f\"|save] Encoded {idx + 1} vocab entries...\")\n",
       "329|\n",
       "330|        if self.verbose:\n",
       "331|            self.print_func(\n",
       "332|                f\"|save] Finished encoding {len(encoded_vocab_to_str)} vocab entries.\"\n",
       "333|            )\n",
       "334|\n",
       "335|        # Save vocab.json (encoded_string -> token_id)\n",
       "336|        encoder = {encoded_vocab_to_str|token_id]: token_id for token_id in vocab}\n",
       "337|        vocab_path = f\"{base_path}_vocab.json\"\n",
       "338|        if self.verbose:\n",
       "339|            self.print_func(\n",
       "340|                f\"|save] Writing vocab to {vocab_path} (encoded_string -> token_id)...\"\n",
       "341|            )\n",
       "342|        with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
       "343|            json.dump(encoder, f, ensure_ascii=False, indent=2)\n",
       "344|        if self.verbose:\n",
       "345|            self.print_func(\n",
       "346|                f\"|save] Wrote vocab to {vocab_path} (entries: {len(encoder)}).\"\n",
       "347|            )\n",
       "348|\n",
       "349|        # Save merges.txt using encoded strings\n",
       "350|        merges_path = f\"{base_path}_merges.txt\"\n",
       "351|        if self.verbose:\n",
       "352|            self.print_func(\n",
       "353|                f\"|save] Writing merges to {merges_path} (human-readable encoded pairs)...\"\n",
       "354|            )\n",
       "355|        with open(merges_path, \"w\", encoding=\"utf-8\") as f:\n",
       "356|            f.write(f\"#version: {version}\\n\")\n",
       "357|            for i, (pair, token_id) in enumerate(merged_pairs.items()):\n",
       "358|                token1_encoded = encoded_vocab_to_str|pair|0]]\n",
       "359|                token2_encoded = encoded_vocab_to_str|pair|1]]\n",
       "360|                f.write(f\"{token1_encoded} {token2_encoded}\\n\")\n",
       "361|\n",
       "362|                # small progress prints for merges\n",
       "363|                if self.verbose and i < 5:\n",
       "364|                    self.print_func(\n",
       "365|                        f\"|save] Merge example {i + 1}: {token1_encoded} + {token2_encoded} -> \n",
       "token_id={token_id}\"\n",
       "366|                    )\n",
       "367|\n",
       "368|        if self.verbose:\n",
       "369|            self.print_func(\n",
       "370|                f\"|save] Wrote merges to {merges_path} (entries: {len(merged_pairs)}).\"\n",
       "371|            )\n",
       "372|            self.print_func(\"|save] Completed save().\")\n",
       "373|\n",
       "374|    def load(self, base_path: str):\n",
       "375|        if self.verbose:\n",
       "376|            self.print_func(f\"|load] Starting load() -> base_path: {base_path}\")\n",
       "377|\n",
       "378|        byte_decoder = self._get_byte_decoder()\n",
       "379|\n",
       "380|        # Load vocab.json\n",
       "381|        vocab_path = f\"{base_path}_vocab.json\"\n",
       "382|        if self.verbose:\n",
       "383|            self.print_func(f\"|load] Reading vocab file: {vocab_path}\")\n",
       "384|        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
       "385|            encoder = json.load(f)\n",
       "386|        if self.verbose:\n",
       "387|            self.print_func(f\"|load] Loaded encoder with {len(encoder)} entries.\")\n",
       "388|\n",
       "389|        # Decode back to original strings\n",
       "390|        vocab_to_str = {}\n",
       "391|        for idx, (encoded_str, token_id) in enumerate(encoder.items()):\n",
       "392|            # Decode from encoded representation back to original string\n",
       "393|            decoded_bytes = bytes(|byte_decoder|c] for c in encoded_str])\n",
       "394|            original_str = decoded_bytes.decode(\"utf-8\")\n",
       "395|            vocab_to_str|token_id] = original_str\n",
       "396|\n",
       "397|            if self.verbose:\n",
       "398|                if idx < 5:\n",
       "399|                    self.print_func(\n",
       "400|                        f\"|load] Decoded example {idx+1}: token_id={token_id} -> {original_str!r}\"\n",
       "401|                    )\n",
       "402|                elif (idx + 1) % 1000 == 0:\n",
       "403|                    self.print_func(f\"|load] Decoded {idx+1} vocab entries...\")\n",
       "404|\n",
       "405|        vocab = sorted(vocab_to_str.keys())\n",
       "406|        if self.verbose:\n",
       "407|            self.print_func(f\"|load] Finished decoding vocab (size={len(vocab)}).\")\n",
       "408|\n",
       "409|        # Load merges.txt\n",
       "410|        merges_path = f\"{base_path}_merges.txt\"\n",
       "411|        if self.verbose:\n",
       "412|            self.print_func(f\"|load] Reading merges file: {merges_path}\")\n",
       "413|        merged_pairs = {}\n",
       "414|        next_token = max(vocab) + 1\n",
       "415|\n",
       "416|        with open(merges_path, \"r\", encoding=\"utf-8\") as f:\n",
       "417|            lines = f.readlines()|1:]  # Skip header\n",
       "418|            for i, line in enumerate(lines):\n",
       "419|                if line.strip():\n",
       "420|                    token1_str, token2_str = line.strip().split(\" \", 1)\n",
       "421|                    # Find token IDs for the encoded strings\n",
       "422|                    token1_id = encoder|token1_str]\n",
       "423|                    token2_id = encoder|token2_str]\n",
       "424|                    merged_pairs|(token1_id, token2_id)] = encoder|\n",
       "425|                        token1_str + token2_str\n",
       "426|                    ]\n",
       "427|                    next_token += 1\n",
       "428|\n",
       "429|                    if self.verbose and i < 5:\n",
       "430|                        self.print_func(\n",
       "431|                            f\"|load] Merge example {i+1}: {token1_str}+{token2_str} \"\n",
       "432|                            f\"-> token_id={merged_pairs|(token1_id, token2_id)]}\"\n",
       "433|                        )\n",
       "434|\n",
       "435|        if self.verbose:\n",
       "436|            self.print_func(f\"|load] Loaded {len(merged_pairs)} merge rules.\")\n",
       "437|            self.print_func(\"|load] Completed load().\")\n",
       "438|\n",
       "439|        self.vocab = vocab\n",
       "440|        self.vocab_to_str = vocab_to_str\n",
       "441|        self.merged_pairs = merged_pairs\n",
       "442|        self._token_id_mapping()\n",
       "443|\n",
       "444|        return vocab, vocab_to_str, merged_pairs\n",
       "445|\n",
       "446|    def encode(\n",
       "447|        self,\n",
       "448|        text: str,\n",
       "449|        normalise: bool = False,\n",
       "450|    ) -> List|int]:\n",
       "451|        \"\"\"\n",
       "452|        Encode text to BPE tokens using learned merged pairs.\n",
       "453|        Args:\n",
       "454|            text: Input string\n",
       "455|            merged_pairs: Dict mapping pairs to new tokens\n",
       "456|            splitting_pattern: Compiled regex splitting pattern\n",
       "457|            vocab_to_str: Dict mapping token id to string representation\n",
       "458|        Returns:\n",
       "459|            List of int tokens\n",
       "460|        \"\"\"\n",
       "461|\n",
       "462|        # Create reverse mapping for special tokens (only tokens starting with < and ending with >)\n",
       "463|        special_token_to_id = {\n",
       "464|            token_str: token_id\n",
       "465|            for token_id, token_str in self.vocab_to_str.items()\n",
       "466|            if token_str in self.special_tokens\n",
       "467|        }\n",
       "468|        sorted_pairs = self.merged_pairs.items()\n",
       "469|\n",
       "470|        # handle special tokens by making a regex pattern to escape them\n",
       "471|        special_patterns = |re.escape(token) for token in special_token_to_id.keys()]\n",
       "472|        special_regex = \"(\" + \"|\".join(special_patterns) + \")\"\n",
       "473|        segments = re.split(special_regex, text)\n",
       "474|        self.verbose and self.print_func(f\"{len(segments) = }\")\n",
       "475|        all_tokens = |]\n",
       "476|        for segment in tqdm(segments, position=0):\n",
       "477|            if segment in special_token_to_id:\n",
       "478|                # This segment is a special token - add its ID directly\n",
       "479|                all_tokens.append(special_token_to_id|segment])\n",
       "480|            elif segment:  # Non-empty regular text segment\n",
       "481|                subwords_tokens = self._split_data(segment)\n",
       "482|                for pair, new_token in tqdm(sorted_pairs, position=1, leave=False):\n",
       "483|                    subwords_tokens, _ = self._merge(subwords_tokens, pair, new_token)\n",
       "484|                segment_tokens = |\n",
       "485|                    token for subword in subwords_tokens for token in subword\n",
       "486|                ]\n",
       "487|                all_tokens.extend(segment_tokens)\n",
       "488|        return all_tokens\n",
       "489|\n",
       "490|    def decode(self, tokens: List|int]) -> str:\n",
       "491|        \"\"\"\n",
       "492|        Decode list of BPE tokens back to string\n",
       "493|        Args:\n",
       "494|            tokens: List of token ids\n",
       "495|            vocab_to_str: Dict mapping token id to string\n",
       "496|        Returns:\n",
       "497|            Decoded string\n",
       "498|        \"\"\"\n",
       "499|        # Map each token to its string representation and concatenate\n",
       "500|        return \"\".join(self.vocab_to_str|token] for token in tqdm(tokens))\n",
       "501|\n",
       "502|    def tokenize(\n",
       "503|        self, texts: list|str], padding_direction: str = \"left\", max_length: int = 128\n",
       "504|    ):\n",
       "505|        # since we dont have the vocab number for the special tokens\n",
       "506|        # this is a hacky way to get that\n",
       "507|        # then we can use it in token_to_id\n",
       "508|        special_token_to_id = {\n",
       "509|            token_str: token_id\n",
       "510|            for token_id, token_str in self.vocab_to_str.items()\n",
       "511|            if token_str.startswith(\"<\") and token_str.endswith(\">\")\n",
       "512|        }\n",
       "513|        pad_id = self.token_to_id|special_token_to_id|self.pad_token]]\n",
       "514|        encoded = |self.encode(t)|:max_length] for t in texts]\n",
       "515|        # pre allocate numpy arrays\n",
       "516|        batch_size = len(encoded)\n",
       "517|        input_ids = np.full((batch_size, max_length), pad_id, dtype=np.int32)\n",
       "518|        attention_mask = np.zeros((batch_size, max_length), dtype=np.int32)\n",
       "519|        # fill in the input_ids and attention_mask\n",
       "520|        for i, seq in enumerate(encoded):\n",
       "521|            length = len(seq)\n",
       "522|            if padding_direction == \"right\":\n",
       "523|                input_ids|i, :length] = seq\n",
       "524|                attention_mask|i, :length] = 1\n",
       "525|            elif padding_direction == \"left\":\n",
       "526|                input_ids|i, -length:] = seq\n",
       "527|                attention_mask|i, -length:] = 1\n",
       "528|            else:\n",
       "529|                raise ValueError(\"padding_direction must be 'left' or 'right'\")\n",
       "530|\n",
       "531|        return input_ids, attention_mask\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 1.20 seconds| Input tokens: 12,534 | Output tokens: 44]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 1.20 seconds| Input tokens: 12,534 | Output tokens: 44]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step 3</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep 3\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The MedievalLMTokenizer accepts the following          │\n",
       "│ parameters in its constructor:\\n\\n- min_merge_threshold: int = 400\\n- max_token_threshold: int = 512\\n-         │\n",
       "│ splitting_pattern: Optional[re.Pattern] = None\\n- special_tokens: Optional[List[str]] = None\\n- text: str =     │\n",
       "│ None\\n- base_path: str = None\\n- verbose: int = 1\\n- logger: logging.Logger = None'}                            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n",
       "</pre>\n"
      ],
      "text/plain": [
       "╭─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n",
       "│ Calling tool: 'final_answer' with arguments: {'answer': 'The MedievalLMTokenizer accepts the following          │\n",
       "│ parameters in its constructor:\\n\\n- min_merge_threshold: int = 400\\n- max_token_threshold: int = 512\\n-         │\n",
       "│ splitting_pattern: Optional[re.Pattern] = None\\n- special_tokens: Optional[List[str]] = None\\n- text: str =     │\n",
       "│ None\\n- base_path: str = None\\n- verbose: int = 1\\n- logger: logging.Logger = None'}                            │\n",
       "╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Observations: The MedievalLMTokenizer accepts the following parameters in its constructor:\n",
       "\n",
       "- min_merge_threshold: int = 400\n",
       "- max_token_threshold: int = 512\n",
       "- splitting_pattern: Optional|re.Pattern] = None\n",
       "- special_tokens: Optional|List|str]] = None\n",
       "- text: str = None\n",
       "- base_path: str = None\n",
       "- verbose: int = 1\n",
       "- logger: logging.Logger = None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Observations: The MedievalLMTokenizer accepts the following parameters in its constructor:\n",
       "\n",
       "- min_merge_threshold: int = 400\n",
       "- max_token_threshold: int = 512\n",
       "- splitting_pattern: Optional|re.Pattern] = None\n",
       "- special_tokens: Optional|List|str]] = None\n",
       "- text: str = None\n",
       "- base_path: str = None\n",
       "- verbose: int = 1\n",
       "- logger: logging.Logger = None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Final answer: The MedievalLMTokenizer accepts the following parameters in its constructor:</span>\n",
       "\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- min_merge_threshold: int = 400</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- max_token_threshold: int = 512</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- splitting_pattern: Optional[re.Pattern] = None</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- special_tokens: Optional[List[str]] = None</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- text: str = None</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- base_path: str = None</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- verbose: int = 1</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">- logger: logging.Logger = None</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mFinal answer: The MedievalLMTokenizer accepts the following parameters in its constructor:\u001b[0m\n",
       "\n",
       "\u001b[1;38;2;212;183;2m- min_merge_threshold: int = 400\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- max_token_threshold: int = 512\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- splitting_pattern: Optional[re.Pattern] = None\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- special_tokens: Optional[List[str]] = None\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- text: str = None\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- base_path: str = None\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- verbose: int = 1\u001b[0m\n",
       "\u001b[1;38;2;212;183;2m- logger: logging.Logger = None\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 3: Duration 2.85 seconds| Input tokens: 29,284 | Output tokens: 153]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 3: Duration 2.85 seconds| Input tokens: 29,284 | Output tokens: 153]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with MCPClient(server_parameters) as tools:\n",
    "    agent = ToolCallingAgent(tools=tools, model=model)\n",
    "    agent.run(\"What parameters does MedievalLMTokenizer accept?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7927a15a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x7fa0e77566d0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7fa0afa79b90>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7fa0afa82190>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7fa0afa83690>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7fa0afa8e150>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x7fa0e7760990>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49e096",
   "metadata": {},
   "source": [
    "# try with llamaindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc1b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the MCP Client (The Bridge)\n",
    "# LlamaIndex has a specific client for stdio connections\n",
    "client = BasicMCPClient(\n",
    "    command_or_url=MCP_BIN,\n",
    "    args=[\"--workspace\", WORKSPACE, \"--lsp\", LSP_BIN, \"--\", \"--stdio\"],\n",
    "    env=os.environ,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5972d198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔌 Connecting to MCP Server...\n",
      "✅ Loaded 6 tools: ['definition', 'diagnostics', 'edit_file', 'hover', 'references', 'rename_symbol']\n"
     ]
    }
   ],
   "source": [
    "# 3. Convert MCP tools into LlamaIndex Tools\n",
    "print(\"🔌 Connecting to MCP Server...\")\n",
    "tool_spec = McpToolSpec(client=client)\n",
    "# This async method fetches the tools (definition, references, etc.)\n",
    "# We need to run this inside an async function or loop\n",
    "# Since Jupyter is async, we can just await it\n",
    "mcp_tools = await tool_spec.to_tool_list_async()\n",
    "\n",
    "print(f\"✅ Loaded {len(mcp_tools)} tools: {[t.metadata.name for t in mcp_tools]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77fa7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Setup the LLM (Hugging Face)\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    context_window=32000,  # Important for large context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b520a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent(tools=mcp_tools, llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc58026a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ReActAgent' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m(\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFind the definition of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhelper_function\u001b[39m\u001b[33m'\u001b[39m\u001b[33m in test_sample.py and tell me what it returns.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ReActAgent' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\n",
    "    \"Find the definition of 'helper_function' in test_sample.py and tell me what it returns.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d58c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create the Agent\n",
    "# ReActAgent is best for tool use\n",
    "agent = FunctionAgent(\n",
    "    name=\"LSP agent\",\n",
    "    description=\"Agent that can use LSP provided tools to analyse code\",\n",
    "    tools=mcp_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    system_prompt=\"You are an world class LSP agent who can make use of the LSP tools provided to analyse and find details about given code.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07bdb891",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = AgentWorkflow(\n",
    "    agents=[agent],\n",
    "    root_agent=\"agent\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fd2c8e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "LLM must be a FunctionCallingLLM",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m workflow.run(\u001b[33m\"\u001b[39m\u001b[33mWhat parameters does MedievalLMTokenizer accept?\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/workflows/runtime/broker.py:162\u001b[39m, in \u001b[36mWorkflowBroker.start.<locals>._run_workflow\u001b[39m\u001b[34m(run_id, tags)\u001b[39m\n\u001b[32m    153\u001b[39m workflow_registry.register_run(\n\u001b[32m    154\u001b[39m     run_id=run_id,\n\u001b[32m    155\u001b[39m     workflow=workflow,\n\u001b[32m   (...)\u001b[39m\u001b[32m    158\u001b[39m     steps=registered.steps,\n\u001b[32m    159\u001b[39m )\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     workflow_result = \u001b[38;5;28;01mawait\u001b[39;00m registered.workflow_function(\n\u001b[32m    163\u001b[39m         start_event,\n\u001b[32m    164\u001b[39m         init_state,\n\u001b[32m    165\u001b[39m         run_id,\n\u001b[32m    166\u001b[39m     )\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# ensure run context is cleaned up even on failure\u001b[39;00m\n\u001b[32m    169\u001b[39m     workflow_registry.delete_run(run_id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/workflows/runtime/control_loop.py:310\u001b[39m, in \u001b[36mcontrol_loop\u001b[39m\u001b[34m(start_event, init_state, run_id)\u001b[39m\n\u001b[32m    306\u001b[39m state = init_state \u001b[38;5;129;01mor\u001b[39;00m BrokerState.from_workflow(current.workflow)\n\u001b[32m    307\u001b[39m runner = _ControlLoopRunner(\n\u001b[32m    308\u001b[39m     current.workflow, current.plugin, current.context, current.steps, state\n\u001b[32m    309\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(start_event=start_event)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/workflows/runtime/control_loop.py:283\u001b[39m, in \u001b[36m_ControlLoopRunner.run\u001b[39m\u001b[34m(self, start_event, start_with_timeout)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m command \u001b[38;5;129;01min\u001b[39;00m commands:\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_command(command)\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    285\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cleanup_tasks()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/workflows/runtime/control_loop.py:202\u001b[39m, in \u001b[36m_ControlLoopRunner.process_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(command, CommandFailWorkflow):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cleanup_tasks()\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m command.exception\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown command type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(command)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/workflows/runtime/types/step_function.py:120\u001b[39m, in \u001b[36mas_step_worker_function.<locals>.wrapper\u001b[39m\u001b[34m(state, step_name, event, context, workflow)\u001b[39m\n\u001b[32m    115\u001b[39m     result: R = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.get_event_loop().run_in_executor(\n\u001b[32m    116\u001b[39m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    117\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m: copy.run(partial_func),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m partial_func()\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Event):\n\u001b[32m    122\u001b[39m         msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(result).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m instead of an Event instance.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/llama_index_instrumentation/dispatcher.py:386\u001b[39m, in \u001b[36mDispatcher.span.<locals>.async_wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28mself\u001b[39m.span_enter(\n\u001b[32m    379\u001b[39m     id_=id_,\n\u001b[32m    380\u001b[39m     bound_args=bound_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    383\u001b[39m     tags=tags,\n\u001b[32m    384\u001b[39m )\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m.event(SpanDropEvent(span_id=id_, err_str=\u001b[38;5;28mstr\u001b[39m(e)))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/llama_index/core/agent/workflow/multi_agent_workflow.py:415\u001b[39m, in \u001b[36mAgentWorkflow.run_agent_step\u001b[39m\u001b[34m(self, ctx, ev)\u001b[39m\n\u001b[32m    412\u001b[39m user_msg_str = \u001b[38;5;28;01mawait\u001b[39;00m ctx.store.get(\u001b[33m\"\u001b[39m\u001b[33muser_msg_str\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    413\u001b[39m tools = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_tools(ev.current_agent_name, user_msg_str \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m agent_output = \u001b[38;5;28;01mawait\u001b[39;00m agent.take_step(\n\u001b[32m    416\u001b[39m     ctx,\n\u001b[32m    417\u001b[39m     ev.input,\n\u001b[32m    418\u001b[39m     tools,\n\u001b[32m    419\u001b[39m     memory,\n\u001b[32m    420\u001b[39m )\n\u001b[32m    422\u001b[39m ctx.write_event_to_stream(agent_output)\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m agent_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/smolagents/.venv/lib/python3.11/site-packages/llama_index/core/agent/workflow/function_agent.py:109\u001b[39m, in \u001b[36mFunctionAgent.take_step\u001b[39m\u001b[34m(self, ctx, llm_input, tools, memory)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Take a single step with the function calling agent.\"\"\"\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm.metadata.is_function_calling_model:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLLM must be a FunctionCallingLLM\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    111\u001b[39m scratchpad: List[ChatMessage] = \u001b[38;5;28;01mawait\u001b[39;00m ctx.store.get(\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mself\u001b[39m.scratchpad_key, default=[]\n\u001b[32m    113\u001b[39m )\n\u001b[32m    114\u001b[39m current_llm_input = [*llm_input, *scratchpad]\n",
      "\u001b[31mValueError\u001b[39m: LLM must be a FunctionCallingLLM"
     ]
    }
   ],
   "source": [
    "response = await workflow.run(\"What parameters does MedievalLMTokenizer accept?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed73c76",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     34\u001b[39m         resp5 = \u001b[38;5;28;01mawait\u001b[39;00m client.call_tool(\n\u001b[32m     35\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreferences\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[33m\"\u001b[39m\u001b[33msymbol\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmy_function\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmax_results\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m50\u001b[39m}\n\u001b[32m     36\u001b[39m         )\n\u001b[32m     37\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mreferences my_function:\u001b[39m\u001b[33m\"\u001b[39m, resp5)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/asyncio/runners.py:186\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    182\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    185\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    187\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75e455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
